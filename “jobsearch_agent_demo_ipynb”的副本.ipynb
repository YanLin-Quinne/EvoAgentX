{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6m35xnU+FlR2R/vBt9TIX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YanLin-Quinne/EvoAgentX/blob/main/%E2%80%9Cjobsearch_agent_demo_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Academic Job Search Tool: User Guide\n",
        "\n",
        "A Systematic Approach to Faculty Position Discovery for Sociology Graduates\n",
        "\n",
        "Overview\n",
        "\n",
        "This computational tool provides a comprehensive solution for identifying academic positions in sociology, with particular emphasis on family studies, childhood research, and care work. The system has been designed specifically for doctoral graduates from Oxford University seeking faculty appointments at leading institutions worldwide.\n",
        "\n",
        "System Design and Scope\n",
        "\n",
        "The search algorithm targets positions across four primary geographical regions:\n",
        "United States: US News top 200 institutions with established sociology programmes\n",
        "United Kingdom: QS top 300 universities, including Russell Group institutions\n",
        "Europe: Leading universities in the Netherlands, Germany, France, Switzerland, and Nordic countries\n",
        "Asia-Pacific: Hong Kong, Singapore, and Sino-foreign cooperative institutions\n"
      ],
      "metadata": {
        "id": "AFxIteyDKRpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Setup and Installation\n",
        "# Run this cell first to install all required packages\n",
        "\n",
        "!pip install requests beautifulsoup4 selenium pandas openpyxl fake-useragent\n",
        "!pip install webdriver-manager lxml html5lib\n",
        "\n",
        "# Import all required libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import random\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import re\n",
        "from fake_useragent import UserAgent\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All required packages installed successfully!\")\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready for academic job scraping configuration.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GITKSS3TDd2R",
        "outputId": "f62998a2-be99-4daa-e2dd-5d1f9ce0ab5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.33.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.11/dist-packages (2.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (2025.4.26)\n",
            "All required packages installed successfully!\n",
            "Libraries imported successfully!\n",
            "Ready for academic job scraping configuration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Target Universities Configuration\n",
        "# Configure target universities based on US News and QS rankings\n",
        "\n",
        "# Top US Universities (US News Top 200) - Strong Sociology Programs\n",
        "US_UNIVERSITIES = [\n",
        "    # Top Tier US Universities\n",
        "    \"Harvard University\", \"Stanford University\", \"Massachusetts Institute of Technology\",\n",
        "    \"University of California Berkeley\", \"University of Chicago\", \"Princeton University\",\n",
        "    \"Yale University\", \"Columbia University\", \"University of Pennsylvania\", \"Northwestern University\",\n",
        "    \"Duke University\", \"Johns Hopkins University\", \"University of California Los Angeles\",\n",
        "    \"Brown University\", \"Cornell University\", \"Rice University\", \"Vanderbilt University\",\n",
        "    \"Washington University in St. Louis\", \"University of Notre Dame\", \"Emory University\",\n",
        "\n",
        "    # Strong Sociology Programs\n",
        "    \"University of Wisconsin Madison\", \"University of Michigan Ann Arbor\", \"New York University\",\n",
        "    \"University of North Carolina Chapel Hill\", \"University of Texas Austin\", \"Boston University\",\n",
        "    \"Georgetown University\", \"Carnegie Mellon University\", \"University of Southern California\",\n",
        "    \"University of California San Diego\", \"University of California Davis\", \"University of Florida\",\n",
        "    \"Ohio State University\", \"Pennsylvania State University\", \"University of Washington\",\n",
        "    \"University of Minnesota Twin Cities\", \"Arizona State University\", \"Rutgers University\",\n",
        "    \"University of Maryland College Park\", \"University of Illinois Urbana-Champaign\",\n",
        "    \"Indiana University Bloomington\", \"University of Virginia\", \"Purdue University\",\n",
        "    \"Michigan State University\", \"University of Iowa\", \"University of California Irvine\"\n",
        "]\n",
        "\n",
        "# QS World Rankings Top 300 - UK Universities\n",
        "UK_UNIVERSITIES = [\n",
        "    # Russell Group and Top UK Universities\n",
        "    \"University of Oxford\", \"University of Cambridge\", \"Imperial College London\",\n",
        "    \"University College London\", \"London School of Economics\", \"King's College London\",\n",
        "    \"University of Edinburgh\", \"University of Manchester\", \"University of Warwick\",\n",
        "    \"University of Bristol\", \"University of Glasgow\", \"Durham University\",\n",
        "    \"University of Sheffield\", \"University of Birmingham\", \"University of Leeds\",\n",
        "    \"University of Southampton\", \"University of York\", \"Lancaster University\",\n",
        "    \"University of Exeter\", \"Cardiff University\", \"Queen Mary University of London\",\n",
        "    \"University of Bath\", \"University of Liverpool\", \"Newcastle University\",\n",
        "    \"University of Nottingham\", \"University of Sussex\", \"Loughborough University\",\n",
        "    \"University of Leicester\", \"University of Reading\", \"Brunel University London\",\n",
        "    \"University of Essex\", \"Goldsmiths University of London\", \"University of Kent\"\n",
        "]\n",
        "\n",
        "# Asia-Pacific Universities (Sino-Foreign Cooperative + Top Regional)\n",
        "ASIA_PACIFIC_UNIVERSITIES = [\n",
        "    # Hong Kong\n",
        "    \"University of Hong Kong\", \"Chinese University of Hong Kong\",\n",
        "    \"Hong Kong University of Science and Technology\", \"City University of Hong Kong\",\n",
        "    \"Hong Kong Polytechnic University\", \"Hong Kong Baptist University\",\n",
        "\n",
        "    # Singapore\n",
        "    \"National University of Singapore\", \"Nanyang Technological University\",\n",
        "\n",
        "    # China - Sino-Foreign Cooperative Universities\n",
        "    \"Shanghai New York University\", \"University of Nottingham Ningbo China\",\n",
        "    \"Xi'an Jiaotong-Liverpool University\", \"Beijing Institute of Technology Zhuhai\",\n",
        "    \"Duke Kunshan University\", \"New York University Shanghai\", \"Wenzhou-Kean University\",\n",
        "    \"The Chinese University of Hong Kong Shenzhen\", \"Kean University at Wenzhou\"\n",
        "]\n",
        "\n",
        "# European Universities (QS Top 300)\n",
        "EUROPEAN_UNIVERSITIES = [\n",
        "    # Netherlands\n",
        "    \"University of Amsterdam\", \"Delft University of Technology\", \"Utrecht University\",\n",
        "    \"Leiden University\", \"Erasmus University Rotterdam\", \"University of Groningen\",\n",
        "    \"VU Amsterdam\", \"Tilburg University\",\n",
        "\n",
        "    # Germany\n",
        "    \"Technical University of Munich\", \"Ludwig Maximilian University of Munich\",\n",
        "    \"Heidelberg University\", \"Humboldt University of Berlin\", \"University of Freiburg\",\n",
        "    \"Free University of Berlin\", \"University of Göttingen\",\n",
        "\n",
        "    # France\n",
        "    \"Sorbonne University\", \"École Normale Supérieure Paris\", \"Sciences Po\",\n",
        "    \"University of Paris\", \"École Polytechnique\",\n",
        "\n",
        "    # Switzerland\n",
        "    \"ETH Zurich\", \"University of Zurich\", \"University of Geneva\", \"University of Lausanne\",\n",
        "\n",
        "    # Nordic Countries\n",
        "    \"University of Copenhagen\", \"Stockholm University\", \"University of Oslo\",\n",
        "    \"University of Helsinki\", \"Lund University\", \"Uppsala University\"\n",
        "]\n",
        "\n",
        "# Combine all target universities\n",
        "ALL_TARGET_UNIVERSITIES = US_UNIVERSITIES + UK_UNIVERSITIES + ASIA_PACIFIC_UNIVERSITIES + EUROPEAN_UNIVERSITIES\n",
        "\n",
        "# Research focus keywords for sociology positions\n",
        "SOCIOLOGY_KEYWORDS = [\n",
        "    \"sociology\", \"social science\", \"family studies\", \"childhood studies\",\n",
        "    \"care work\", \"unpaid care\", \"social policy\", \"demography\", \"social research\",\n",
        "    \"qualitative research\", \"quantitative methods\", \"social theory\",\n",
        "    \"gender studies\", \"migration studies\", \"urban sociology\", \"cultural sociology\",\n",
        "    \"education sociology\", \"medical sociology\", \"digital sociology\", \"ethnography\"\n",
        "]\n",
        "\n",
        "print(f\"UNIVERSITY CONFIGURATION COMPLETE\")\n",
        "print(f\"US Universities: {len(US_UNIVERSITIES)}\")\n",
        "print(f\"UK Universities: {len(UK_UNIVERSITIES)}\")\n",
        "print(f\"Asia-Pacific Universities: {len(ASIA_PACIFIC_UNIVERSITIES)}\")\n",
        "print(f\"European Universities: {len(EUROPEAN_UNIVERSITIES)}\")\n",
        "print(f\"Total target universities: {len(ALL_TARGET_UNIVERSITIES)}\")\n",
        "print(f\"Sociology keywords configured: {len(SOCIOLOGY_KEYWORDS)}\")\n",
        "print(\"Ready for job data structure setup!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yOGt0aDE4Rk",
        "outputId": "61a39fa6-8101-4819-b4ed-a8a5d49feb1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNIVERSITY CONFIGURATION COMPLETE\n",
            "US Universities: 46\n",
            "UK Universities: 33\n",
            "Asia-Pacific Universities: 17\n",
            "European Universities: 30\n",
            "Total target universities: 126\n",
            "Sociology keywords configured: 20\n",
            "Ready for job data structure setup!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Job Data Structure and Relevance Scoring\n",
        "# Define job data structure and intelligent scoring system\n",
        "\n",
        "class AcademicJob:\n",
        "    \"\"\"Data structure for academic job postings\"\"\"\n",
        "\n",
        "    def __init__(self, title=\"\", institution=\"\", location=\"\", country=\"\",\n",
        "                 salary=\"\", deadline=\"\", url=\"\", source_platform=\"\", description=\"\"):\n",
        "        self.title = title\n",
        "        self.institution = institution\n",
        "        self.location = location\n",
        "        self.country = country\n",
        "        self.salary = salary\n",
        "        self.deadline = deadline\n",
        "        self.url = url\n",
        "        self.source_platform = source_platform\n",
        "        self.description = description\n",
        "        self.scraped_date = datetime.now().strftime('%Y-%m-%d')\n",
        "        self.relevance_score = 0\n",
        "\n",
        "def calculate_relevance_score(job_title, job_description, institution):\n",
        "    \"\"\"\n",
        "    Calculate relevance score based on research interests and career level\n",
        "    Optimized for sociology PhD with family/childhood/care work expertise\n",
        "    \"\"\"\n",
        "\n",
        "    score = 0\n",
        "    text_to_analyze = f\"{job_title} {job_description}\".lower()\n",
        "\n",
        "    # HIGH PRIORITY KEYWORDS (Your research specializations)\n",
        "    high_priority_keywords = {\n",
        "        'family': 15, 'childhood': 15, 'care work': 15, 'unpaid care': 15,\n",
        "        'family studies': 18, 'childhood studies': 18, 'care studies': 15,\n",
        "        'sociology': 12, 'social science': 10, 'social policy': 12,\n",
        "        'qualitative research': 10, 'mixed methods': 8, 'ethnography': 10,\n",
        "        'china studies': 15, 'asia': 8, 'comparative': 10, 'cross-cultural': 8\n",
        "    }\n",
        "\n",
        "    # MEDIUM PRIORITY KEYWORDS (Career level and relevant areas)\n",
        "    medium_priority_keywords = {\n",
        "        'assistant professor': 20, 'lecturer': 15, 'senior lecturer': 18,\n",
        "        'tenure track': 18, 'postdoc': 12, 'research fellow': 12,\n",
        "        'social research': 8, 'gender': 8, 'migration': 6, 'urban': 6,\n",
        "        'cultural': 6, 'education': 8, 'health': 6, 'digital': 5,\n",
        "        'interview': 5, 'survey': 5, 'participant observation': 8\n",
        "    }\n",
        "\n",
        "    # LOW PRIORITY BUT RELEVANT KEYWORDS\n",
        "    low_priority_keywords = {\n",
        "        'social theory': 5, 'quantitative': 4, 'statistics': 4,\n",
        "        'community': 4, 'policy': 5, 'welfare': 6, 'social work': 4,\n",
        "        'anthropology': 3, 'psychology': 3, 'economics': 2\n",
        "    }\n",
        "\n",
        "    # Calculate keyword scores\n",
        "    for keyword, points in high_priority_keywords.items():\n",
        "        if keyword in text_to_analyze:\n",
        "            score += points\n",
        "\n",
        "    for keyword, points in medium_priority_keywords.items():\n",
        "        if keyword in text_to_analyze:\n",
        "            score += points\n",
        "\n",
        "    for keyword, points in low_priority_keywords.items():\n",
        "        if keyword in text_to_analyze:\n",
        "            score += points\n",
        "\n",
        "    # INSTITUTION PRESTIGE BONUS\n",
        "    if institution in US_UNIVERSITIES[:20] or institution in UK_UNIVERSITIES[:10]:\n",
        "        score += 10  # Top tier institutions\n",
        "    elif institution in ASIA_PACIFIC_UNIVERSITIES:\n",
        "        score += 8   # Asia-Pacific (your regional expertise)\n",
        "    elif institution in ALL_TARGET_UNIVERSITIES:\n",
        "        score += 5   # Other target institutions\n",
        "\n",
        "    # GEOGRAPHIC PREFERENCE BONUS (based on your background)\n",
        "    if any(keyword in text_to_analyze for keyword in ['uk', 'britain', 'england']):\n",
        "        score += 5  # UK familiarity\n",
        "    if any(keyword in text_to_analyze for keyword in ['china', 'chinese', 'asia']):\n",
        "        score += 8  # Your research expertise region\n",
        "\n",
        "    # PENALTY FOR IRRELEVANT POSITIONS\n",
        "    irrelevant_keywords = [\n",
        "        'engineering', 'computer science', 'physics', 'chemistry',\n",
        "        'biology', 'mathematics', 'business administration', 'finance',\n",
        "        'medical doctor', 'clinical', 'laboratory'\n",
        "    ]\n",
        "    for keyword in irrelevant_keywords:\n",
        "        if keyword in text_to_analyze and 'social' not in text_to_analyze:\n",
        "            score -= 10\n",
        "\n",
        "    return max(0, score)  # Ensure non-negative score\n",
        "\n",
        "def get_priority_tier(score):\n",
        "    \"\"\"Convert relevance score to priority tier for easy filtering\"\"\"\n",
        "    if score >= 40:\n",
        "        return \"High Priority\"\n",
        "    elif score >= 25:\n",
        "        return \"Medium Priority\"\n",
        "    elif score >= 15:\n",
        "        return \"Low Priority\"\n",
        "    else:\n",
        "        return \"Consider\"\n",
        "\n",
        "# Test the scoring system with sample job\n",
        "test_job = AcademicJob(\n",
        "    title=\"Assistant Professor of Sociology - Family and Childhood Studies\",\n",
        "    institution=\"Boston University\",\n",
        "    description=\"Seeking candidates with expertise in family sociology, childhood studies, qualitative research methods, and experience with Chinese family structures\"\n",
        ")\n",
        "\n",
        "test_score = calculate_relevance_score(test_job.title, test_job.description, test_job.institution)\n",
        "test_job.relevance_score = test_score\n",
        "\n",
        "print(\"RELEVANCE SCORING SYSTEM CONFIGURED\")\n",
        "print(f\"Test job: {test_job.title}\")\n",
        "print(f\"Institution: {test_job.institution}\")\n",
        "print(f\"Relevance score: {test_score}\")\n",
        "print(f\"Priority tier: {get_priority_tier(test_score)}\")\n",
        "print(\"Scoring system ready for job evaluation!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ-xKTVxE4Ue",
        "outputId": "4c4ef05b-174e-448a-a9e0-4ea1ced765d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RELEVANCE SCORING SYSTEM CONFIGURED\n",
            "Test job: Assistant Professor of Sociology - Family and Childhood Studies\n",
            "Institution: Boston University\n",
            "Relevance score: 103\n",
            "Priority tier: High Priority\n",
            "Scoring system ready for job evaluation!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 4: Web Scraping Infrastructure\n",
        "# Core scraping infrastructure with error handling and anti-blocking measures\n",
        "\n",
        "class GlobalAcademicJobScraper:\n",
        "    \"\"\"Main scraper class for academic job platforms\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.ua = UserAgent()\n",
        "        self.jobs_found = []\n",
        "        self.failed_urls = []\n",
        "        self.setup_session()\n",
        "        print(\"Academic job scraper initialized!\")\n",
        "\n",
        "    def setup_session(self):\n",
        "        \"\"\"Configure session with realistic headers and settings\"\"\"\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': self.ua.random,\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "            'DNT': '1'\n",
        "        })\n",
        "\n",
        "    def setup_chrome_driver(self):\n",
        "        \"\"\"Setup Chrome driver for JavaScript-heavy sites\"\"\"\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('--disable-gpu')\n",
        "        chrome_options.add_argument('--window-size=1920,1080')\n",
        "        chrome_options.add_argument(f'--user-agent={self.ua.random}')\n",
        "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "\n",
        "        try:\n",
        "            driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
        "            # Remove webdriver property\n",
        "            driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "            return driver\n",
        "        except Exception as e:\n",
        "            print(f\"Chrome driver setup failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def random_delay(self, min_seconds=1, max_seconds=3):\n",
        "        \"\"\"Add random delay to avoid being blocked\"\"\"\n",
        "        delay = random.uniform(min_seconds, max_seconds)\n",
        "        time.sleep(delay)\n",
        "        print(f\"Waiting {delay:.1f} seconds...\")\n",
        "\n",
        "    def safe_get(self, url, use_selenium=False, timeout=10):\n",
        "        \"\"\"Safely fetch webpage content with error handling\"\"\"\n",
        "        try:\n",
        "            if use_selenium:\n",
        "                driver = self.setup_chrome_driver()\n",
        "                if driver:\n",
        "                    driver.get(url)\n",
        "                    time.sleep(random.uniform(2, 4))  # Random wait for page load\n",
        "                    content = driver.page_source\n",
        "                    driver.quit()\n",
        "                    return BeautifulSoup(content, 'html.parser')\n",
        "                else:\n",
        "                    return None\n",
        "            else:\n",
        "                response = self.session.get(url, timeout=timeout)\n",
        "                response.raise_for_status()\n",
        "                return BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request failed for {url}: {e}\")\n",
        "            self.failed_urls.append(url)\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error fetching {url}: {e}\")\n",
        "            self.failed_urls.append(url)\n",
        "            return None\n",
        "\n",
        "    def extract_text_safely(self, element, default=\"N/A\"):\n",
        "        \"\"\"Safely extract text from BeautifulSoup element\"\"\"\n",
        "        if element:\n",
        "            text = element.get_text(strip=True)\n",
        "            return text if text else default\n",
        "        return default\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and normalize text data\"\"\"\n",
        "        if not text or text == \"N/A\":\n",
        "            return \"N/A\"\n",
        "\n",
        "        # Remove extra whitespace and normalize\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "        # Remove problematic characters\n",
        "        text = re.sub(r'[^\\w\\s\\-.,():/&]', '', text)\n",
        "        return text[:500]  # Limit length to prevent issues\n",
        "\n",
        "    def parse_deadline(self, deadline_text):\n",
        "        \"\"\"Parse deadline text into standardized format\"\"\"\n",
        "        if not deadline_text or deadline_text == \"N/A\":\n",
        "            return \"N/A\"\n",
        "\n",
        "        # Common deadline patterns\n",
        "        patterns = [\n",
        "            r'(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})',  # MM/DD/YYYY or DD/MM/YYYY\n",
        "            r'(\\d{1,2}\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\w*\\s+\\d{4})',\n",
        "            r'((Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\w*\\s+\\d{1,2},?\\s+\\d{4})',\n",
        "            r'(\\d{4}-\\d{2}-\\d{2})'  # YYYY-MM-DD\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, deadline_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "\n",
        "        return self.clean_text(deadline_text)\n",
        "\n",
        "    def get_status_summary(self):\n",
        "        \"\"\"Get current scraping status\"\"\"\n",
        "        return {\n",
        "            'jobs_found': len(self.jobs_found),\n",
        "            'failed_urls': len(self.failed_urls),\n",
        "            'high_priority': len([job for job in self.jobs_found if job.relevance_score >= 40]),\n",
        "            'medium_priority': len([job for job in self.jobs_found if 25 <= job.relevance_score < 40])\n",
        "        }\n",
        "\n",
        "# Initialize the global scraper instance\n",
        "scraper = GlobalAcademicJobScraper()\n",
        "\n",
        "print(\"WEB SCRAPING INFRASTRUCTURE READY\")\n",
        "print(f\"User agent: {scraper.ua.random[:50]}...\")\n",
        "print(\"Anti-blocking measures configured\")\n",
        "print(\"Chrome driver ready for dynamic sites\")\n",
        "print(\"Ready to scrape academic job platforms!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnTMDb7QHEf8",
        "outputId": "e59360e5-a3ef-4d86-84b2-31584d10105c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Academic job scraper initialized!\n",
            "WEB SCRAPING INFRASTRUCTURE READY\n",
            "User agent: Mozilla/5.0 (iPhone; CPU iPhone OS 18_4 like Mac O...\n",
            "Anti-blocking measures configured\n",
            "Chrome driver ready for dynamic sites\n",
            "Ready to scrape academic job platforms!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 5: Jobs.ac.uk Scraper Implementation\n",
        "# UK's premier academic job platform scraper\n",
        "\n",
        "def scrape_jobs_ac_uk(scraper, keywords=None, max_pages=5):\n",
        "    \"\"\"\n",
        "    Scrape Jobs.ac.uk for sociology positions\n",
        "    Primary source for UK academic positions\n",
        "    \"\"\"\n",
        "\n",
        "    if keywords is None:\n",
        "        keywords = [\"sociology\", \"social science\", \"family studies\"]\n",
        "\n",
        "    base_url = \"https://www.jobs.ac.uk\"\n",
        "    jobs_found = []\n",
        "\n",
        "    print(\"STARTING JOBS.AC.UK SCRAPING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for keyword in keywords:\n",
        "        print(f\"\\nSearching for: '{keyword}'\")\n",
        "\n",
        "        for page in range(1, max_pages + 1):\n",
        "            search_url = f\"{base_url}/search?keywords={keyword.replace(' ', '+')}&page={page}\"\n",
        "            print(f\"   Page {page}\")\n",
        "\n",
        "            soup = scraper.safe_get(search_url)\n",
        "            if not soup:\n",
        "                print(f\"   Failed to load page {page}\")\n",
        "                continue\n",
        "\n",
        "            # Multiple selector strategies for job listings\n",
        "            job_selectors = [\n",
        "                'div.job-result',\n",
        "                'div.job-listing',\n",
        "                'article.job',\n",
        "                'div.result',\n",
        "                'li.job',\n",
        "                'tr.job-row'\n",
        "            ]\n",
        "\n",
        "            job_listings = []\n",
        "            for selector in job_selectors:\n",
        "                job_listings = soup.select(selector)\n",
        "                if job_listings:\n",
        "                    break\n",
        "\n",
        "            if not job_listings:\n",
        "                print(f\"   No jobs found on page {page}\")\n",
        "                if page == 1:\n",
        "                    # Try alternative approach for first page\n",
        "                    all_links = soup.find_all('a', href=True)\n",
        "                    job_links = [link for link in all_links if '/job/' in link.get('href', '')]\n",
        "                    if job_links:\n",
        "                        print(f\"   Found {len(job_links)} job links via alternative method\")\n",
        "                        job_listings = [link.parent for link in job_links[:10]]  # Limit to 10\n",
        "\n",
        "                if not job_listings:\n",
        "                    break\n",
        "\n",
        "            print(f\"   Found {len(job_listings)} job listings\")\n",
        "\n",
        "            for job_element in job_listings:\n",
        "                try:\n",
        "                    # Extract job title\n",
        "                    title_selectors = ['h3 a', 'h2 a', 'a.job-title', '.job-title', 'h3', 'h2']\n",
        "                    title = \"\"\n",
        "                    title_link = \"\"\n",
        "\n",
        "                    for selector in title_selectors:\n",
        "                        title_elem = job_element.select_one(selector)\n",
        "                        if title_elem:\n",
        "                            title = scraper.extract_text_safely(title_elem)\n",
        "                            if title_elem.name == 'a':\n",
        "                                title_link = title_elem.get('href', '')\n",
        "                            elif title_elem.find('a'):\n",
        "                                title_link = title_elem.find('a').get('href', '')\n",
        "                            break\n",
        "\n",
        "                    if not title or len(title) < 5:\n",
        "                        continue\n",
        "\n",
        "                    # Get full job URL\n",
        "                    job_url = urljoin(base_url, title_link) if title_link else \"\"\n",
        "\n",
        "                    # Extract institution\n",
        "                    institution_selectors = ['.employer', '.institution', '.company', 'p.employer']\n",
        "                    institution = \"\"\n",
        "                    for selector in institution_selectors:\n",
        "                        inst_elem = job_element.select_one(selector)\n",
        "                        if inst_elem:\n",
        "                            institution = scraper.extract_text_safely(inst_elem)\n",
        "                            break\n",
        "\n",
        "                    # Extract location\n",
        "                    location_selectors = ['.location', 'span.location', 'p.location']\n",
        "                    location = \"\"\n",
        "                    for selector in location_selectors:\n",
        "                        loc_elem = job_element.select_one(selector)\n",
        "                        if loc_elem:\n",
        "                            location = scraper.extract_text_safely(loc_elem)\n",
        "                            break\n",
        "\n",
        "                    # Extract salary\n",
        "                    salary_selectors = ['.salary', 'span.salary', 'p.salary']\n",
        "                    salary = \"\"\n",
        "                    for selector in salary_selectors:\n",
        "                        sal_elem = job_element.select_one(selector)\n",
        "                        if sal_elem:\n",
        "                            salary = scraper.extract_text_safely(sal_elem)\n",
        "                            break\n",
        "\n",
        "                    # Extract deadline\n",
        "                    deadline_selectors = ['.deadline', '.closing-date', 'time', '.date']\n",
        "                    deadline = \"\"\n",
        "                    for selector in deadline_selectors:\n",
        "                        dead_elem = job_element.select_one(selector)\n",
        "                        if dead_elem:\n",
        "                            deadline = scraper.parse_deadline(scraper.extract_text_safely(dead_elem))\n",
        "                            break\n",
        "\n",
        "                    # Determine country based on institution and location\n",
        "                    country = \"United Kingdom\"\n",
        "                    if any(asia_uni in institution for asia_uni in ASIA_PACIFIC_UNIVERSITIES):\n",
        "                        if \"Hong Kong\" in institution or \"Hong Kong\" in location:\n",
        "                            country = \"Hong Kong\"\n",
        "                        elif \"Singapore\" in institution or \"Singapore\" in location:\n",
        "                            country = \"Singapore\"\n",
        "                        else:\n",
        "                            country = \"China\"\n",
        "\n",
        "                    # Create job object\n",
        "                    job = AcademicJob(\n",
        "                        title=scraper.clean_text(title),\n",
        "                        institution=scraper.clean_text(institution),\n",
        "                        location=scraper.clean_text(location),\n",
        "                        country=country,\n",
        "                        salary=scraper.clean_text(salary),\n",
        "                        deadline=deadline,\n",
        "                        url=job_url,\n",
        "                        source_platform=\"Jobs.ac.uk\"\n",
        "                    )\n",
        "\n",
        "                    # Calculate relevance score\n",
        "                    job.relevance_score = calculate_relevance_score(job.title, \"\", job.institution)\n",
        "\n",
        "                    # Filter: only add if relevant or from target universities\n",
        "                    if job.relevance_score > 10 or any(uni in job.institution for uni in ALL_TARGET_UNIVERSITIES):\n",
        "                        jobs_found.append(job)\n",
        "                        priority = get_priority_tier(job.relevance_score)\n",
        "                        print(f\"   Added {priority}: {job.title[:50]}... at {job.institution}\")\n",
        "                        print(f\"      Score: {job.relevance_score} | Deadline: {job.deadline}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"   Error processing job: {e}\")\n",
        "                    continue\n",
        "\n",
        "            scraper.random_delay(2, 4)  # Delay between pages\n",
        "\n",
        "        scraper.random_delay(3, 5)  # Delay between keywords\n",
        "\n",
        "    # Add to main scraper results\n",
        "    scraper.jobs_found.extend(jobs_found)\n",
        "\n",
        "    print(f\"\\nJOBS.AC.UK SCRAPING COMPLETE\")\n",
        "    print(f\"Found {len(jobs_found)} relevant positions\")\n",
        "    print(f\"High priority: {len([job for job in jobs_found if job.relevance_score >= 40])}\")\n",
        "    print(f\"Medium priority: {len([job for job in jobs_found if 25 <= job.relevance_score < 40])}\")\n",
        "\n",
        "    return jobs_found\n",
        "\n",
        "# Test the Jobs.ac.uk scraper\n",
        "print(\"TESTING JOBS.AC.UK SCRAPER\")\n",
        "print(\"Running limited test search...\")\n",
        "test_jobs = scrape_jobs_ac_uk(scraper, keywords=[\"sociology\"], max_pages=1)\n",
        "print(f\"Test completed! Found {len(test_jobs)} jobs in test search.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVR0foA9HEk_",
        "outputId": "bc7d8a3c-7d53-4424-e233-75a74721f0ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TESTING JOBS.AC.UK SCRAPER\n",
            "Running limited test search...\n",
            "STARTING JOBS.AC.UK SCRAPING\n",
            "==================================================\n",
            "\n",
            "Searching for: 'sociology'\n",
            "   Page 1\n",
            "   No jobs found on page 1\n",
            "   Found 25 job links via alternative method\n",
            "   Found 10 job listings\n",
            "Waiting 3.9 seconds...\n",
            "Waiting 4.1 seconds...\n",
            "\n",
            "JOBS.AC.UK SCRAPING COMPLETE\n",
            "Found 0 relevant positions\n",
            "High priority: 0\n",
            "Medium priority: 0\n",
            "Test completed! Found 0 jobs in test search.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 6: HigherEdJobs and Times Higher Education Scrapers\n",
        "# US and international academic job platform scrapers\n",
        "\n",
        "def scrape_higher_ed_jobs(scraper, keywords=None, max_pages=3):\n",
        "    \"\"\"\n",
        "    Scrape HigherEdJobs.com for US academic positions\n",
        "    Major platform for American university positions\n",
        "    \"\"\"\n",
        "\n",
        "    if keywords is None:\n",
        "        keywords = [\"sociology\", \"social+science\"]\n",
        "\n",
        "    base_url = \"https://www.higheredjobs.com\"\n",
        "    jobs_found = []\n",
        "\n",
        "    print(\"STARTING HIGHEREDJOBS.COM SCRAPING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for keyword in keywords:\n",
        "        print(f\"\\nSearching for: '{keyword}'\")\n",
        "\n",
        "        for page in range(1, max_pages + 1):\n",
        "            # Faculty positions in social sciences\n",
        "            search_url = f\"{base_url}/search/advanced_action.cfm?JobCat=18&Keywords={keyword}&PosType=1&NumJobs=25&PageNum={page}\"\n",
        "            print(f\"   Page {page}\")\n",
        "\n",
        "            soup = scraper.safe_get(search_url)\n",
        "            if not soup:\n",
        "                print(f\"   Failed to load page {page}\")\n",
        "                continue\n",
        "\n",
        "            # Find job listings - HigherEdJobs uses table format\n",
        "            job_selectors = [\n",
        "                'tr.data-row-even, tr.data-row-odd',\n",
        "                'div.job-listing',\n",
        "                'tr[class*=\"row\"]',\n",
        "                'article.job'\n",
        "            ]\n",
        "\n",
        "            job_listings = []\n",
        "            for selector in job_selectors:\n",
        "                job_listings = soup.select(selector)\n",
        "                if job_listings:\n",
        "                    break\n",
        "\n",
        "            if not job_listings:\n",
        "                print(f\"   No jobs found on page {page}\")\n",
        "                break\n",
        "\n",
        "            print(f\"   Found {len(job_listings)} job listings\")\n",
        "\n",
        "            for job_element in job_listings:\n",
        "                try:\n",
        "                    # Extract job title\n",
        "                    title_selectors = ['a.jobTitle', 'td.jobTitle a', '.job-title a', 'h3 a']\n",
        "                    title = \"\"\n",
        "                    job_url = \"\"\n",
        "\n",
        "                    for selector in title_selectors:\n",
        "                        title_elem = job_element.select_one(selector)\n",
        "                        if title_elem:\n",
        "                            title = scraper.extract_text_safely(title_elem)\n",
        "                            job_url = urljoin(base_url, title_elem.get('href', ''))\n",
        "                            break\n",
        "\n",
        "                    if not title or len(title) < 5:\n",
        "                        continue\n",
        "\n",
        "                    # Extract institution\n",
        "                    institution_selectors = ['td.institution', '.employer', '.institution']\n",
        "                    institution = \"\"\n",
        "                    for selector in institution_selectors:\n",
        "                        inst_elem = job_element.select_one(selector)\n",
        "                        if inst_elem:\n",
        "                            institution = scraper.extract_text_safely(inst_elem)\n",
        "                            break\n",
        "\n",
        "                    # Extract location\n",
        "                    location_selectors = ['td.location', '.location']\n",
        "                    location = \"\"\n",
        "                    for selector in location_selectors:\n",
        "                        loc_elem = job_element.select_one(selector)\n",
        "                        if loc_elem:\n",
        "                            location = scraper.extract_text_safely(loc_elem)\n",
        "                            break\n",
        "\n",
        "                    # Extract deadline\n",
        "                    deadline_selectors = ['td.deadline', '.deadline', '.closing-date']\n",
        "                    deadline = \"\"\n",
        "                    for selector in deadline_selectors:\n",
        "                        dead_elem = job_element.select_one(selector)\n",
        "                        if dead_elem:\n",
        "                            deadline = scraper.parse_deadline(scraper.extract_text_safely(dead_elem))\n",
        "                            break\n",
        "\n",
        "                    # Create job object\n",
        "                    job = AcademicJob(\n",
        "                        title=scraper.clean_text(title),\n",
        "                        institution=scraper.clean_text(institution),\n",
        "                        location=scraper.clean_text(location),\n",
        "                        country=\"United States\",\n",
        "                        salary=\"N/A\",\n",
        "                        deadline=deadline,\n",
        "                        url=job_url,\n",
        "                        source_platform=\"HigherEdJobs\"\n",
        "                    )\n",
        "\n",
        "                    # Calculate relevance score\n",
        "                    job.relevance_score = calculate_relevance_score(job.title, \"\", job.institution)\n",
        "\n",
        "                    # Filter for relevant positions\n",
        "                    if job.relevance_score > 10 or any(uni in job.institution for uni in US_UNIVERSITIES):\n",
        "                        jobs_found.append(job)\n",
        "                        priority = get_priority_tier(job.relevance_score)\n",
        "                        print(f\"   Added {priority}: {job.title[:50]}... at {job.institution}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"   Error processing job: {e}\")\n",
        "                    continue\n",
        "\n",
        "            scraper.random_delay(2, 4)\n",
        "\n",
        "        scraper.random_delay(3, 5)\n",
        "\n",
        "    scraper.jobs_found.extend(jobs_found)\n",
        "    print(f\"\\nHIGHEREDJOBS SCRAPING COMPLETE\")\n",
        "    print(f\"Found {len(jobs_found)} relevant positions\")\n",
        "    return jobs_found\n",
        "\n",
        "def scrape_times_higher_education(scraper, keywords=None, max_pages=3):\n",
        "    \"\"\"\n",
        "    Scrape Times Higher Education Jobs\n",
        "    International academic positions platform\n",
        "    \"\"\"\n",
        "\n",
        "    if keywords is None:\n",
        "        keywords = [\"sociology\", \"social science\"]\n",
        "\n",
        "    base_url = \"https://www.timeshighereducation.com\"\n",
        "    jobs_url = f\"{base_url}/unijobs\"\n",
        "    jobs_found = []\n",
        "\n",
        "    print(\"STARTING TIMES HIGHER EDUCATION SCRAPING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for keyword in keywords:\n",
        "        print(f\"\\nSearching for: '{keyword}'\")\n",
        "\n",
        "        # THE Jobs search URL structure\n",
        "        search_url = f\"{jobs_url}/listings?search={keyword.replace(' ', '%20')}\"\n",
        "\n",
        "        soup = scraper.safe_get(search_url, use_selenium=True)  # May need Selenium for dynamic content\n",
        "        if not soup:\n",
        "            print(f\"   Failed to load THE Jobs search\")\n",
        "            continue\n",
        "\n",
        "        # Find job listings\n",
        "        job_selectors = [\n",
        "            'article.job-listing',\n",
        "            'div.job-item',\n",
        "            '.job-result',\n",
        "            'div[class*=\"job\"]'\n",
        "        ]\n",
        "\n",
        "        job_listings = []\n",
        "        for selector in job_selectors:\n",
        "            job_listings = soup.select(selector)\n",
        "            if job_listings:\n",
        "                break\n",
        "\n",
        "        if not job_listings:\n",
        "            print(f\"   No jobs found for '{keyword}'\")\n",
        "            continue\n",
        "\n",
        "        print(f\"   Found {len(job_listings)} job listings\")\n",
        "\n",
        "        for job_element in job_listings[:20]:  # Limit to avoid overloading\n",
        "            try:\n",
        "                # Extract job title\n",
        "                title_selectors = ['h3 a', 'h2 a', '.job-title a', 'h3', 'h2']\n",
        "                title = \"\"\n",
        "                job_url = \"\"\n",
        "\n",
        "                for selector in title_selectors:\n",
        "                    title_elem = job_element.select_one(selector)\n",
        "                    if title_elem:\n",
        "                        title = scraper.extract_text_safely(title_elem)\n",
        "                        if title_elem.name == 'a':\n",
        "                            job_url = urljoin(base_url, title_elem.get('href', ''))\n",
        "                        elif title_elem.find('a'):\n",
        "                            job_url = urljoin(base_url, title_elem.find('a').get('href', ''))\n",
        "                        break\n",
        "\n",
        "                if not title or len(title) < 5:\n",
        "                    continue\n",
        "\n",
        "                # Extract institution\n",
        "                institution_selectors = ['.employer', '.institution', '.company']\n",
        "                institution = \"\"\n",
        "                for selector in institution_selectors:\n",
        "                    inst_elem = job_element.select_one(selector)\n",
        "                    if inst_elem:\n",
        "                        institution = scraper.extract_text_safely(inst_elem)\n",
        "                        break\n",
        "\n",
        "                # Extract location\n",
        "                location_selectors = ['.location', 'span.location']\n",
        "                location = \"\"\n",
        "                for selector in location_selectors:\n",
        "                    loc_elem = job_element.select_one(selector)\n",
        "                    if loc_elem:\n",
        "                        location = scraper.extract_text_safely(loc_elem)\n",
        "                        break\n",
        "\n",
        "                # Extract salary\n",
        "                salary_selectors = ['.salary', 'span.salary']\n",
        "                salary = \"\"\n",
        "                for selector in salary_selectors:\n",
        "                    sal_elem = job_element.select_one(selector)\n",
        "                    if sal_elem:\n",
        "                        salary = scraper.extract_text_safely(sal_elem)\n",
        "                        break\n",
        "\n",
        "                # Determine country based on institution and location\n",
        "                country = \"United Kingdom\"  # Default for THE\n",
        "                if any(us_uni in institution for us_uni in US_UNIVERSITIES):\n",
        "                    country = \"United States\"\n",
        "                elif any(asia_uni in institution for asia_uni in ASIA_PACIFIC_UNIVERSITIES):\n",
        "                    if \"Hong Kong\" in institution or \"Hong Kong\" in location:\n",
        "                        country = \"Hong Kong\"\n",
        "                    elif \"Singapore\" in institution or \"Singapore\" in location:\n",
        "                        country = \"Singapore\"\n",
        "                    else:\n",
        "                        country = \"Asia-Pacific\"\n",
        "                elif any(eu_uni in institution for eu_uni in EUROPEAN_UNIVERSITIES):\n",
        "                    country = \"Europe\"\n",
        "\n",
        "                # Create job object\n",
        "                job = AcademicJob(\n",
        "                    title=scraper.clean_text(title),\n",
        "                    institution=scraper.clean_text(institution),\n",
        "                    location=scraper.clean_text(location),\n",
        "                    country=country,\n",
        "                    salary=scraper.clean_text(salary),\n",
        "                    deadline=\"N/A\",\n",
        "                    url=job_url,\n",
        "                    source_platform=\"Times Higher Education\"\n",
        "                )\n",
        "\n",
        "                # Calculate relevance score\n",
        "                job.relevance_score = calculate_relevance_score(job.title, \"\", job.institution)\n",
        "\n",
        "                # Filter for relevant positions\n",
        "                if job.relevance_score > 10 or any(uni in job.institution for uni in ALL_TARGET_UNIVERSITIES):\n",
        "                    jobs_found.append(job)\n",
        "                    priority = get_priority_tier(job.relevance_score)\n",
        "                    print(f\"   Added {priority}: {job.title[:50]}... at {job.institution}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Error processing job: {e}\")\n",
        "                continue\n",
        "\n",
        "        scraper.random_delay(3, 5)\n",
        "\n",
        "    scraper.jobs_found.extend(jobs_found)\n",
        "    print(f\"\\nTIMES HIGHER EDUCATION SCRAPING COMPLETE\")\n",
        "    print(f\"Found {len(jobs_found)} relevant positions\")\n",
        "    return jobs_found\n",
        "\n",
        "print(\"US AND INTERNATIONAL JOB SCRAPERS CONFIGURED\")\n",
        "print(\"HigherEdJobs.com scraper ready for US positions\")\n",
        "print(\"Times Higher Education scraper ready for international positions\")\n",
        "print(\"Ready to search major academic job platforms!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFSvw2V8HEmI",
        "outputId": "cc922db4-ce46-44cd-c030-5986f7b41fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "US AND INTERNATIONAL JOB SCRAPERS CONFIGURED\n",
            "HigherEdJobs.com scraper ready for US positions\n",
            "Times Higher Education scraper ready for international positions\n",
            "Ready to search major academic job platforms!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 7: Academic Positions and University Career Pages\n",
        "# European academic platform and direct university scraping\n",
        "\n",
        "def scrape_academic_positions_org(scraper, keywords=None, max_pages=3):\n",
        "    \"\"\"\n",
        "    Scrape AcademicPositions.com (European focus)\n",
        "    Major platform for European university positions\n",
        "    \"\"\"\n",
        "\n",
        "    if keywords is None:\n",
        "        keywords = [\"sociology\", \"social science\"]\n",
        "\n",
        "    base_url = \"https://academicpositions.com\"\n",
        "    jobs_found = []\n",
        "\n",
        "    print(\"STARTING ACADEMICPOSITIONS.COM SCRAPING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for keyword in keywords:\n",
        "        print(f\"\\nSearching for: '{keyword}'\")\n",
        "\n",
        "        for page in range(1, max_pages + 1):\n",
        "            search_url = f\"{base_url}/ad/search?q={keyword.replace(' ', '+')}&page={page}\"\n",
        "            print(f\"   Page {page}\")\n",
        "\n",
        "            soup = scraper.safe_get(search_url)\n",
        "            if not soup:\n",
        "                print(f\"   Failed to load page {page}\")\n",
        "                continue\n",
        "\n",
        "            # Find job listings\n",
        "            job_selectors = [\n",
        "                'div.position-item',\n",
        "                'article.job-ad',\n",
        "                '.job-listing',\n",
        "                'div[class*=\"position\"]',\n",
        "                'div[class*=\"job\"]'\n",
        "            ]\n",
        "\n",
        "            job_listings = []\n",
        "            for selector in job_selectors:\n",
        "                job_listings = soup.select(selector)\n",
        "                if job_listings:\n",
        "                    break\n",
        "\n",
        "            if not job_listings:\n",
        "                print(f\"   No jobs found on page {page}\")\n",
        "                break\n",
        "\n",
        "            print(f\"   Found {len(job_listings)} job listings\")\n",
        "\n",
        "            for job_element in job_listings:\n",
        "                try:\n",
        "                    # Extract job title and URL\n",
        "                    title_selectors = ['h3 a', 'h2 a', 'a.job-title', '.title a', 'h3', 'h2']\n",
        "                    title = \"\"\n",
        "                    job_url = \"\"\n",
        "\n",
        "                    for selector in title_selectors:\n",
        "                        title_elem = job_element.select_one(selector)\n",
        "                        if title_elem:\n",
        "                            title = scraper.extract_text_safely(title_elem)\n",
        "                            if title_elem.name == 'a':\n",
        "                                job_url = urljoin(base_url, title_elem.get('href', ''))\n",
        "                            elif title_elem.find('a'):\n",
        "                                job_url = urljoin(base_url, title_elem.find('a').get('href', ''))\n",
        "                            break\n",
        "\n",
        "                    if not title or len(title) < 5:\n",
        "                        continue\n",
        "\n",
        "                    # Extract institution\n",
        "                    institution_selectors = ['.employer', '.institution', '.company', '.university']\n",
        "                    institution = \"\"\n",
        "                    for selector in institution_selectors:\n",
        "                        inst_elem = job_element.select_one(selector)\n",
        "                        if inst_elem:\n",
        "                            institution = scraper.extract_text_safely(inst_elem)\n",
        "                            break\n",
        "\n",
        "                    # Extract location\n",
        "                    location_selectors = ['.location', 'span.location', '.place']\n",
        "                    location = \"\"\n",
        "                    for selector in location_selectors:\n",
        "                        loc_elem = job_element.select_one(selector)\n",
        "                        if loc_elem:\n",
        "                            location = scraper.extract_text_safely(loc_elem)\n",
        "                            break\n",
        "\n",
        "                    # Determine country based on location\n",
        "                    country = \"Europe\"  # Default\n",
        "                    location_lower = location.lower()\n",
        "                    if \"uk\" in location_lower or \"united kingdom\" in location_lower or \"england\" in location_lower or \"scotland\" in location_lower:\n",
        "                        country = \"United Kingdom\"\n",
        "                    elif \"netherlands\" in location_lower or \"amsterdam\" in location_lower or \"utrecht\" in location_lower:\n",
        "                        country = \"Netherlands\"\n",
        "                    elif \"germany\" in location_lower or \"berlin\" in location_lower or \"munich\" in location_lower:\n",
        "                        country = \"Germany\"\n",
        "                    elif \"france\" in location_lower or \"paris\" in location_lower:\n",
        "                        country = \"France\"\n",
        "                    elif \"switzerland\" in location_lower or \"zurich\" in location_lower or \"geneva\" in location_lower:\n",
        "                        country = \"Switzerland\"\n",
        "                    elif \"sweden\" in location_lower or \"stockholm\" in location_lower:\n",
        "                        country = \"Sweden\"\n",
        "                    elif \"denmark\" in location_lower or \"copenhagen\" in location_lower:\n",
        "                        country = \"Denmark\"\n",
        "                    elif \"norway\" in location_lower or \"oslo\" in location_lower:\n",
        "                        country = \"Norway\"\n",
        "\n",
        "                    # Extract deadline\n",
        "                    deadline_selectors = ['.deadline', '.closing-date', 'time', '.date']\n",
        "                    deadline = \"\"\n",
        "                    for selector in deadline_selectors:\n",
        "                        dead_elem = job_element.select_one(selector)\n",
        "                        if dead_elem:\n",
        "                            deadline = scraper.parse_deadline(scraper.extract_text_safely(dead_elem))\n",
        "                            break\n",
        "\n",
        "                    # Create job object\n",
        "                    job = AcademicJob(\n",
        "                        title=scraper.clean_text(title),\n",
        "                        institution=scraper.clean_text(institution),\n",
        "                        location=scraper.clean_text(location),\n",
        "                        country=country,\n",
        "                        salary=\"N/A\",\n",
        "                        deadline=deadline,\n",
        "                        url=job_url,\n",
        "                        source_platform=\"AcademicPositions.com\"\n",
        "                    )\n",
        "\n",
        "                    # Calculate relevance score\n",
        "                    job.relevance_score = calculate_relevance_score(job.title, \"\", job.institution)\n",
        "\n",
        "                    # Filter for relevant positions\n",
        "                    if job.relevance_score > 10 or any(uni in job.institution for uni in EUROPEAN_UNIVERSITIES):\n",
        "                        jobs_found.append(job)\n",
        "                        priority = get_priority_tier(job.relevance_score)\n",
        "                        print(f\"   Added {priority}: {job.title[:50]}... at {job.institution}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"   Error processing job: {e}\")\n",
        "                    continue\n",
        "\n",
        "            scraper.random_delay(2, 4)\n",
        "\n",
        "        scraper.random_delay(3, 5)\n",
        "\n",
        "    scraper.jobs_found.extend(jobs_found)\n",
        "    print(f\"\\nACADEMICPOSITIONS.COM SCRAPING COMPLETE\")\n",
        "    print(f\"Found {len(jobs_found)} relevant positions\")\n",
        "    return jobs_found\n",
        "\n",
        "def scrape_university_career_pages(scraper, target_universities=None, max_unis=10):\n",
        "    \"\"\"\n",
        "    Scrape direct university career pages for top institutions\n",
        "    Direct access to university HR systems\n",
        "    \"\"\"\n",
        "\n",
        "    if target_universities is None:\n",
        "        target_universities = US_UNIVERSITIES[:5] + UK_UNIVERSITIES[:5]  # Top 5 from each\n",
        "\n",
        "    # University career page patterns (real URLs)\n",
        "    career_page_patterns = {\n",
        "        \"Harvard University\": \"https://sjobs.brassring.com/TGnewUI/Search/Home/HomeWithPreLoad?partnerid=25240\",\n",
        "        \"Stanford University\": \"https://careersearch.stanford.edu/jobs\",\n",
        "        \"University of Oxford\": \"https://www.jobs.ox.ac.uk/home\",\n",
        "        \"University of Cambridge\": \"https://www.jobs.cam.ac.uk/job/\",\n",
        "        \"MIT\": \"https://careers.peopleclick.com/careerscp/client_mit/external/search.do\",\n",
        "        \"Boston University\": \"https://www.bu.edu/careers/jobs/\",\n",
        "        \"University College London\": \"https://www.ucl.ac.uk/human-resources/jobs\",\n",
        "        \"London School of Economics\": \"https://jobs.lse.ac.uk/\",\n",
        "        \"University of Edinburgh\": \"https://www.ed.ac.uk/jobs\",\n",
        "        \"University of Manchester\": \"https://www.manchester.ac.uk/discover/jobs/\",\n",
        "        \"Yale University\": \"https://your.yale.edu/work-yale/find-job\",\n",
        "        \"Columbia University\": \"https://jobs.columbia.edu/\",\n",
        "        \"University of Chicago\": \"https://jobs.uchicago.edu/\",\n",
        "        \"Northwestern University\": \"https://www.northwestern.edu/hr/careers/\"\n",
        "    }\n",
        "\n",
        "    jobs_found = []\n",
        "    print(\"STARTING UNIVERSITY CAREER PAGES SCRAPING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    processed_count = 0\n",
        "    for university in target_universities:\n",
        "        if processed_count >= max_unis:\n",
        "            break\n",
        "\n",
        "        if university not in career_page_patterns:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nScraping {university}...\")\n",
        "        career_url = career_page_patterns[university]\n",
        "\n",
        "        soup = scraper.safe_get(career_url, use_selenium=True)\n",
        "        if not soup:\n",
        "            print(f\"   Failed to access {university} careers page\")\n",
        "            continue\n",
        "\n",
        "        # Generic job listing selectors (varies by university)\n",
        "        job_selectors = [\n",
        "            'div.job-listing', 'tr.job-row', 'article.job',\n",
        "            'div.position', 'li.job-item', 'div.vacancy',\n",
        "            '.job-result', 'div[class*=\"job\"]', 'tr[class*=\"row\"]'\n",
        "        ]\n",
        "\n",
        "        job_listings = []\n",
        "        for selector in job_selectors:\n",
        "            job_listings = soup.select(selector)\n",
        "            if job_listings:\n",
        "                break\n",
        "\n",
        "        if not job_listings:\n",
        "            print(f\"   No job listings found for {university}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"   Found {len(job_listings)} potential positions\")\n",
        "\n",
        "        for job_element in job_listings[:15]:  # Limit to first 15 jobs per university\n",
        "            try:\n",
        "                # Extract title\n",
        "                title_selectors = ['h3', 'h2', 'a.job-title', '.title', '.job-name', 'td.title']\n",
        "                title = \"\"\n",
        "                job_url = career_url  # Default to main page\n",
        "\n",
        "                for selector in title_selectors:\n",
        "                    title_elem = job_element.select_one(selector)\n",
        "                    if title_elem:\n",
        "                        title = scraper.extract_text_safely(title_elem)\n",
        "                        # Try to get specific job URL\n",
        "                        link_elem = title_elem if title_elem.name == 'a' else title_elem.find('a')\n",
        "                        if link_elem and link_elem.get('href'):\n",
        "                            job_url = urljoin(career_url, link_elem.get('href'))\n",
        "                        break\n",
        "\n",
        "                if not title or len(title) < 5:\n",
        "                    continue\n",
        "\n",
        "                # Quick relevance check before processing further\n",
        "                title_lower = title.lower()\n",
        "                relevant_keywords = [\n",
        "                    'sociology', 'social', 'lecturer', 'professor', 'research',\n",
        "                    'family', 'childhood', 'care', 'policy', 'faculty'\n",
        "                ]\n",
        "\n",
        "                if not any(keyword in title_lower for keyword in relevant_keywords):\n",
        "                    continue\n",
        "\n",
        "                # Extract additional details if available\n",
        "                location_elem = job_element.select_one('.location, .place, .campus')\n",
        "                location = scraper.extract_text_safely(location_elem) if location_elem else \"N/A\"\n",
        "\n",
        "                # Create job object\n",
        "                job = AcademicJob(\n",
        "                    title=scraper.clean_text(title),\n",
        "                    institution=university,\n",
        "                    location=scraper.clean_text(location),\n",
        "                    country=\"United States\" if university in US_UNIVERSITIES else \"United Kingdom\",\n",
        "                    salary=\"N/A\",\n",
        "                    deadline=\"N/A\",\n",
        "                    url=job_url,\n",
        "                    source_platform=f\"{university} Careers\"\n",
        "                )\n",
        "\n",
        "                # Calculate relevance score\n",
        "                job.relevance_score = calculate_relevance_score(job.title, \"\", job.institution)\n",
        "\n",
        "                # Add if relevant (higher threshold for direct university scraping)\n",
        "                if job.relevance_score > 15:\n",
        "                    jobs_found.append(job)\n",
        "                    priority = get_priority_tier(job.relevance_score)\n",
        "                    print(f\"   Added {priority}: {job.title[:50]}...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        processed_count += 1\n",
        "        scraper.random_delay(5, 8)  # Longer delay for university sites to be respectful\n",
        "\n",
        "    scraper.jobs_found.extend(jobs_found)\n",
        "    print(f\"\\nUNIVERSITY CAREER PAGES SCRAPING COMPLETE\")\n",
        "    print(f\"Found {len(jobs_found)} relevant positions from university websites\")\n",
        "    return jobs_found\n",
        "\n",
        "print(\"EUROPEAN AND UNIVERSITY SCRAPERS CONFIGURED\")\n",
        "print(\"AcademicPositions.com scraper ready for European positions\")\n",
        "print(\"University career page scraper ready for direct institutional access\")\n",
        "print(\"Ready to access premium academic job sources!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQizYgdWHEoK",
        "outputId": "fe7c69a0-751c-45ac-a5ce-b30e89e54d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EUROPEAN AND UNIVERSITY SCRAPERS CONFIGURED\n",
            "AcademicPositions.com scraper ready for European positions\n",
            "University career page scraper ready for direct institutional access\n",
            "Ready to access premium academic job sources!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 8: Asia-Pacific Scraping and Main Search Orchestrator\n",
        "# Complete search coordination and Asia-Pacific specialized scraping\n",
        "\n",
        "def scrape_asia_pacific_positions(scraper, keywords=None):\n",
        "    \"\"\"\n",
        "    Scrape Asia-Pacific university positions from various sources\n",
        "    Focus on Hong Kong, Singapore, and Sino-foreign cooperative universities\n",
        "    \"\"\"\n",
        "\n",
        "    if keywords is None:\n",
        "        keywords = [\"sociology\", \"social science\"]\n",
        "\n",
        "    jobs_found = []\n",
        "    print(\"STARTING ASIA-PACIFIC POSITIONS SCRAPING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Asia-Pacific specific job boards and approaches\n",
        "    asia_sources = [\n",
        "        {\n",
        "            \"name\": \"Academic Jobs Singapore\",\n",
        "            \"search_terms\": [\"singapore\", \"NUS\", \"NTU\"],\n",
        "            \"country\": \"Singapore\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Hong Kong Universities\",\n",
        "            \"search_terms\": [\"hong kong\", \"HKU\", \"CUHK\", \"HKUST\"],\n",
        "            \"country\": \"Hong Kong\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Sino-Foreign Universities\",\n",
        "            \"search_terms\": [\"NYU Shanghai\", \"Nottingham Ningbo\", \"XJTLU\", \"Duke Kunshan\"],\n",
        "            \"country\": \"China\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Try to find positions through international job boards with Asia-Pacific focus\n",
        "    for source in asia_sources:\n",
        "        print(f\"\\nSearching for {source['name']} positions...\")\n",
        "\n",
        "        # Search through already scraped results for Asia-Pacific institutions\n",
        "        asia_jobs = []\n",
        "        for job in scraper.jobs_found:\n",
        "            if any(term.lower() in job.institution.lower() or term.lower() in job.location.lower()\n",
        "                   for term in source['search_terms']):\n",
        "                asia_jobs.append(job)\n",
        "\n",
        "        print(f\"   Found {len(asia_jobs)} positions for {source['name']}\")\n",
        "        jobs_found.extend(asia_jobs)\n",
        "\n",
        "    # Additionally, look for specific Sino-foreign universities in scraped data\n",
        "    sino_foreign_unis = [\n",
        "        \"Shanghai New York University\", \"NYU Shanghai\",\n",
        "        \"University of Nottingham Ningbo\", \"Nottingham Ningbo\",\n",
        "        \"Xi'an Jiaotong-Liverpool University\", \"XJTLU\",\n",
        "        \"Duke Kunshan University\", \"Wenzhou-Kean University\",\n",
        "        \"Chinese University of Hong Kong Shenzhen\"\n",
        "    ]\n",
        "\n",
        "    additional_asia_jobs = []\n",
        "    for job in scraper.jobs_found:\n",
        "        for uni in sino_foreign_unis:\n",
        "            if uni.lower() in job.institution.lower():\n",
        "                if job not in jobs_found:  # Avoid duplicates\n",
        "                    additional_asia_jobs.append(job)\n",
        "                    job.country = \"China\"  # Update country for Sino-foreign unis\n",
        "                    break\n",
        "\n",
        "    jobs_found.extend(additional_asia_jobs)\n",
        "\n",
        "    print(f\"\\nASIA-PACIFIC SCRAPING COMPLETE\")\n",
        "    print(f\"Identified {len(jobs_found)} Asia-Pacific positions\")\n",
        "    print(f\"Singapore positions: {len([j for j in jobs_found if j.country == 'Singapore'])}\")\n",
        "    print(f\"Hong Kong positions: {len([j for j in jobs_found if j.country == 'Hong Kong'])}\")\n",
        "    print(f\"China/Sino-foreign positions: {len([j for j in jobs_found if j.country == 'China'])}\")\n",
        "\n",
        "    return jobs_found\n",
        "\n",
        "def run_comprehensive_job_search(scraper, comprehensive=True):\n",
        "    \"\"\"\n",
        "    Run comprehensive job search across all platforms\n",
        "    Main orchestrator function\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"STARTING COMPREHENSIVE ACADEMIC JOB SEARCH\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Target: Sociology positions at top universities worldwide\")\n",
        "    print(f\"Research focus: Family studies, childhood, care work\")\n",
        "    print(f\"Level: Oxford sociology PhD\")\n",
        "    print(f\"Search time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Comprehensive mode: {comprehensive}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Clear previous results\n",
        "    scraper.jobs_found = []\n",
        "    scraper.failed_urls = []\n",
        "\n",
        "    # Define search parameters optimized for sociology\n",
        "    sociology_keywords = [\"sociology\", \"social science\", \"family studies\", \"childhood studies\"]\n",
        "\n",
        "    search_phases = []\n",
        "\n",
        "    try:\n",
        "        # Phase 1: Jobs.ac.uk (UK academic jobs - primary source)\n",
        "        print(\"\\nPHASE 1: JOBS.AC.UK\")\n",
        "        print(\"=\"*40)\n",
        "        phase1_jobs = scrape_jobs_ac_uk(scraper, keywords=sociology_keywords, max_pages=4)\n",
        "        search_phases.append((\"Jobs.ac.uk\", len(phase1_jobs)))\n",
        "\n",
        "        # Phase 2: HigherEdJobs (US academic positions)\n",
        "        print(\"\\nPHASE 2: HIGHEREDJOBS.COM\")\n",
        "        print(\"=\"*40)\n",
        "        phase2_jobs = scrape_higher_ed_jobs(scraper, keywords=[\"sociology\", \"social+science\"], max_pages=3)\n",
        "        search_phases.append((\"HigherEdJobs\", len(phase2_jobs)))\n",
        "\n",
        "        # Phase 3: Times Higher Education (International)\n",
        "        print(\"\\nPHASE 3: TIMES HIGHER EDUCATION\")\n",
        "        print(\"=\"*40)\n",
        "        phase3_jobs = scrape_times_higher_education(scraper, keywords=sociology_keywords, max_pages=2)\n",
        "        search_phases.append((\"Times Higher Education\", len(phase3_jobs)))\n",
        "\n",
        "        # Phase 4: AcademicPositions.com (European focus)\n",
        "        print(\"\\nPHASE 4: ACADEMICPOSITIONS.COM\")\n",
        "        print(\"=\"*40)\n",
        "        phase4_jobs = scrape_academic_positions_org(scraper, keywords=sociology_keywords, max_pages=3)\n",
        "        search_phases.append((\"AcademicPositions.com\", len(phase4_jobs)))\n",
        "\n",
        "        if comprehensive:\n",
        "            # Phase 5: University Career Pages (Premium institutions)\n",
        "            print(\"\\nPHASE 5: UNIVERSITY CAREER PAGES\")\n",
        "            print(\"=\"*40)\n",
        "            target_unis = (US_UNIVERSITIES[:8] + UK_UNIVERSITIES[:8] +\n",
        "                          [\"University of Hong Kong\", \"National University of Singapore\"])\n",
        "            phase5_jobs = scrape_university_career_pages(scraper, target_universities=target_unis, max_unis=10)\n",
        "            search_phases.append((\"University Careers\", len(phase5_jobs)))\n",
        "\n",
        "            # Phase 6: Asia-Pacific Focus\n",
        "            print(\"\\nPHASE 6: ASIA-PACIFIC SPECIALIZATION\")\n",
        "            print(\"=\"*40)\n",
        "            phase6_jobs = scrape_asia_pacific_positions(scraper, keywords=sociology_keywords)\n",
        "            search_phases.append((\"Asia-Pacific\", len(phase6_jobs)))\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\nSearch interrupted by user.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\nUnexpected error during search: {e}\")\n",
        "\n",
        "    # Process and deduplicate results\n",
        "    print(\"\\nPROCESSING AND ANALYZING RESULTS\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # Remove duplicates based on title and institution combination\n",
        "    unique_jobs = []\n",
        "    seen_combinations = set()\n",
        "\n",
        "    for job in scraper.jobs_found:\n",
        "        # Create a unique identifier for each job\n",
        "        combination = f\"{job.title.lower().strip()}||{job.institution.lower().strip()}\"\n",
        "        if combination not in seen_combinations:\n",
        "            unique_jobs.append(job)\n",
        "            seen_combinations.add(combination)\n",
        "        else:\n",
        "            print(f\"   Removed duplicate: {job.title[:40]}... at {job.institution}\")\n",
        "\n",
        "    scraper.jobs_found = unique_jobs\n",
        "\n",
        "    # Sort by relevance score (highest first)\n",
        "    scraper.jobs_found.sort(key=lambda x: x.relevance_score, reverse=True)\n",
        "\n",
        "    # Generate comprehensive statistics\n",
        "    total_jobs = len(scraper.jobs_found)\n",
        "    high_priority = len([job for job in scraper.jobs_found if job.relevance_score >= 40])\n",
        "    medium_priority = len([job for job in scraper.jobs_found if 25 <= job.relevance_score < 40])\n",
        "    low_priority = len([job for job in scraper.jobs_found if 15 <= job.relevance_score < 25])\n",
        "\n",
        "    # Geographic and institutional analysis\n",
        "    country_counts = {}\n",
        "    platform_counts = {}\n",
        "    institution_counts = {}\n",
        "\n",
        "    for job in scraper.jobs_found:\n",
        "        country_counts[job.country] = country_counts.get(job.country, 0) + 1\n",
        "        platform_counts[job.source_platform] = platform_counts.get(job.source_platform, 0) + 1\n",
        "        if job.institution in ALL_TARGET_UNIVERSITIES:\n",
        "            institution_counts[job.institution] = institution_counts.get(job.institution, 0) + 1\n",
        "\n",
        "    # Display comprehensive results\n",
        "    print(f\"\\nCOMPREHENSIVE JOB SEARCH COMPLETE!\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"SUMMARY STATISTICS:\")\n",
        "    print(f\"   Total unique positions found: {total_jobs}\")\n",
        "    print(f\"   High priority (40+ score): {high_priority}\")\n",
        "    print(f\"   Medium priority (25-39 score): {medium_priority}\")\n",
        "    print(f\"   Low priority (15-24 score): {low_priority}\")\n",
        "    print(f\"   Failed URLs: {len(scraper.failed_urls)}\")\n",
        "\n",
        "    print(f\"\\nGEOGRAPHIC DISTRIBUTION:\")\n",
        "    for country, count in sorted(country_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"   {country}: {count} positions\")\n",
        "\n",
        "    print(f\"\\nPLATFORM PERFORMANCE:\")\n",
        "    for platform, count in sorted(platform_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"   {platform}: {count} positions\")\n",
        "\n",
        "    print(f\"\\nTOP TARGET INSTITUTIONS:\")\n",
        "    for inst, count in sorted(institution_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "        print(f\"   {inst}: {count} positions\")\n",
        "\n",
        "    print(f\"\\nTOP 10 HIGHEST SCORING POSITIONS:\")\n",
        "    for i, job in enumerate(scraper.jobs_found[:10], 1):\n",
        "        priority = get_priority_tier(job.relevance_score)\n",
        "        print(f\"   {i}. {priority} | Score: {job.relevance_score}\")\n",
        "        print(f\"      {job.title}\")\n",
        "        print(f\"      {job.institution} | {job.country}\")\n",
        "        print(f\"      Deadline: {job.deadline} | Platform: {job.source_platform}\")\n",
        "        print()\n",
        "\n",
        "    print(f\"\\nSEARCH PHASE RESULTS:\")\n",
        "    for phase_name, job_count in search_phases:\n",
        "        print(f\"   {phase_name}: {job_count} positions\")\n",
        "\n",
        "    return scraper.jobs_found\n",
        "\n",
        "def filter_jobs_by_criteria(jobs, min_score=15, target_countries=None, target_institutions=None):\n",
        "    \"\"\"Filter jobs based on specific criteria for targeted applications\"\"\"\n",
        "\n",
        "    if target_countries is None:\n",
        "        target_countries = [\"United Kingdom\", \"United States\", \"Singapore\", \"Hong Kong\", \"China\"]\n",
        "\n",
        "    filtered_jobs = []\n",
        "\n",
        "    print(f\"FILTERING JOBS BY CRITERIA\")\n",
        "    print(f\"   Minimum score: {min_score}\")\n",
        "    print(f\"   Target countries: {', '.join(target_countries)}\")\n",
        "\n",
        "    for job in jobs:\n",
        "        # Score filter\n",
        "        if job.relevance_score < min_score:\n",
        "            continue\n",
        "\n",
        "        # Country filter\n",
        "        if target_countries and job.country not in target_countries:\n",
        "            continue\n",
        "\n",
        "        # Institution filter (if specified)\n",
        "        if target_institutions:\n",
        "            if not any(inst.lower() in job.institution.lower() for inst in target_institutions):\n",
        "                continue\n",
        "\n",
        "        filtered_jobs.append(job)\n",
        "\n",
        "    print(f\"Filtered to {len(filtered_jobs)} positions matching criteria\")\n",
        "    return filtered_jobs\n",
        "\n",
        "def analyze_job_market_trends(jobs):\n",
        "    \"\"\"Analyze trends in the scraped job data for market insights\"\"\"\n",
        "\n",
        "    if not jobs:\n",
        "        return {}\n",
        "\n",
        "    analysis = {\n",
        "        'total_positions': len(jobs),\n",
        "        'avg_relevance_score': sum(job.relevance_score for job in jobs) / len(jobs),\n",
        "        'position_types': {},\n",
        "        'geographic_distribution': {},\n",
        "        'institution_prestige': {'top_tier': 0, 'mid_tier': 0, 'other': 0},\n",
        "        'application_urgency': {'urgent': 0, 'soon': 0, 'later': 0, 'unknown': 0}\n",
        "    }\n",
        "\n",
        "    # Analyze position types\n",
        "    for job in jobs:\n",
        "        title_lower = job.title.lower()\n",
        "        if 'assistant professor' in title_lower:\n",
        "            analysis['position_types']['Assistant Professor'] = analysis['position_types'].get('Assistant Professor', 0) + 1\n",
        "        elif 'lecturer' in title_lower or 'senior lecturer' in title_lower:\n",
        "            analysis['position_types']['Lecturer'] = analysis['position_types'].get('Lecturer', 0) + 1\n",
        "        elif 'professor' in title_lower and 'assistant' not in title_lower:\n",
        "            analysis['position_types']['Professor'] = analysis['position_types'].get('Professor', 0) + 1\n",
        "        elif 'postdoc' in title_lower or 'post-doc' in title_lower:\n",
        "            analysis['position_types']['Postdoc'] = analysis['position_types'].get('Postdoc', 0) + 1\n",
        "        elif 'research' in title_lower:\n",
        "            analysis['position_types']['Research Position'] = analysis['position_types'].get('Research Position', 0) + 1\n",
        "        else:\n",
        "            analysis['position_types']['Other'] = analysis['position_types'].get('Other', 0) + 1\n",
        "\n",
        "    # Geographic distribution\n",
        "    for job in jobs:\n",
        "        analysis['geographic_distribution'][job.country] = analysis['geographic_distribution'].get(job.country, 0) + 1\n",
        "\n",
        "    # Institution prestige analysis\n",
        "    for job in jobs:\n",
        "        if job.institution in (US_UNIVERSITIES[:20] + UK_UNIVERSITIES[:10] +\n",
        "                              [\"University of Hong Kong\", \"National University of Singapore\"]):\n",
        "            analysis['institution_prestige']['top_tier'] += 1\n",
        "        elif job.institution in ALL_TARGET_UNIVERSITIES:\n",
        "            analysis['institution_prestige']['mid_tier'] += 1\n",
        "        else:\n",
        "            analysis['institution_prestige']['other'] += 1\n",
        "\n",
        "    # Application urgency (based on deadlines)\n",
        "    current_date = datetime.now()\n",
        "    for job in jobs:\n",
        "        if job.deadline == \"N/A\":\n",
        "            analysis['application_urgency']['unknown'] += 1\n",
        "        else:\n",
        "            # Simple deadline analysis - would need more robust parsing for production\n",
        "            try:\n",
        "                if any(urgent in job.deadline.lower() for urgent in ['asap', 'urgent', 'immediate']):\n",
        "                    analysis['application_urgency']['urgent'] += 1\n",
        "                elif any(month in job.deadline.lower() for month in ['jan', 'feb', 'mar']):\n",
        "                    analysis['application_urgency']['soon'] += 1\n",
        "                else:\n",
        "                    analysis['application_urgency']['later'] += 1\n",
        "            except:\n",
        "                analysis['application_urgency']['unknown'] += 1\n",
        "\n",
        "    return analysis\n",
        "\n",
        "print(\"MAIN SEARCH ORCHESTRATOR CONFIGURED\")\n",
        "print(\"Comprehensive job search function ready\")\n",
        "print(\"Asia-Pacific specialization scraper ready\")\n",
        "print(\"Job filtering and analysis functions ready\")\n",
        "print(\"Ready to execute full academic job search!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "169ai8BzHEpr",
        "outputId": "7480c64f-b3a9-4eb9-8126-85c5b279f019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAIN SEARCH ORCHESTRATOR CONFIGURED\n",
            "Comprehensive job search function ready\n",
            "Asia-Pacific specialization scraper ready\n",
            "Job filtering and analysis functions ready\n",
            "Ready to execute full academic job search!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 9: Results Export and Report Generation\n",
        "# Comprehensive reporting and export functionality\n",
        "\n",
        "def save_results(scraper, filename_prefix=\"sociology_academic_jobs\"):\n",
        "    \"\"\"\n",
        "    Export results to Excel with multiple sheets and detailed analysis\n",
        "    Creates downloadable Excel file with organized job data\n",
        "    \"\"\"\n",
        "\n",
        "    if not scraper.jobs_found:\n",
        "        print(\"No job data to save\")\n",
        "        return \"\"\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    excel_filename = f\"{filename_prefix}_{timestamp}.xlsx\"\n",
        "\n",
        "    try:\n",
        "        print(\"CREATING EXCEL EXPORT\")\n",
        "        print(\"=\"*30)\n",
        "\n",
        "        # Prepare main job data\n",
        "        jobs_data = []\n",
        "        for job in scraper.jobs_found:\n",
        "            jobs_data.append({\n",
        "                'Relevance Score': job.relevance_score,\n",
        "                'Priority Tier': get_priority_tier(job.relevance_score),\n",
        "                'Job Title': job.title,\n",
        "                'Institution': job.institution,\n",
        "                'Location': job.location,\n",
        "                'Country': job.country,\n",
        "                'Salary Information': job.salary,\n",
        "                'Application Deadline': job.deadline,\n",
        "                'Job URL': job.url,\n",
        "                'Source Platform': job.source_platform,\n",
        "                'Date Scraped': job.scraped_date,\n",
        "                'Target University': 'Yes' if job.institution in ALL_TARGET_UNIVERSITIES else 'No'\n",
        "            })\n",
        "\n",
        "        df_all = pd.DataFrame(jobs_data)\n",
        "\n",
        "        # Create filtered dataframes for different priority levels\n",
        "        df_high_priority = df_all[df_all['Relevance Score'] >= 40]\n",
        "        df_medium_priority = df_all[(df_all['Relevance Score'] >= 25) & (df_all['Relevance Score'] < 40)]\n",
        "        df_low_priority = df_all[(df_all['Relevance Score'] >= 15) & (df_all['Relevance Score'] < 25)]\n",
        "        df_target_institutions = df_all[df_all['Target University'] == 'Yes']\n",
        "\n",
        "        # Create geographic and career level filters\n",
        "        df_uk_positions = df_all[df_all['Country'] == 'United Kingdom']\n",
        "        df_us_positions = df_all[df_all['Country'] == 'United States']\n",
        "        df_asia_positions = df_all[df_all['Country'].isin(['Hong Kong', 'Singapore', 'China'])]\n",
        "\n",
        "        # Assistant professor positions (primary target)\n",
        "        df_assistant_prof = df_all[df_all['Job Title'].str.contains('Assistant Professor', case=False, na=False)]\n",
        "        df_lecturer = df_all[df_all['Job Title'].str.contains('Lecturer', case=False, na=False)]\n",
        "\n",
        "        # Create analysis summary\n",
        "        analysis = analyze_job_market_trends(scraper.jobs_found)\n",
        "\n",
        "        # Prepare analysis data for Excel\n",
        "        analysis_data = [\n",
        "            ['MARKET ANALYSIS SUMMARY', ''],\n",
        "            ['Generated', datetime.now().strftime('%Y-%m-%d %H:%M:%S')],\n",
        "            ['', ''],\n",
        "            ['OVERALL STATISTICS', ''],\n",
        "            ['Total Positions Found', analysis['total_positions']],\n",
        "            ['Average Relevance Score', f\"{analysis['avg_relevance_score']:.2f}\"],\n",
        "            ['High Priority Positions (40+)', len(df_high_priority)],\n",
        "            ['Medium Priority Positions (25-39)', len(df_medium_priority)],\n",
        "            ['Low Priority Positions (15-24)', len(df_low_priority)],\n",
        "            ['', ''],\n",
        "            ['POSITION TYPE DISTRIBUTION', ''],\n",
        "        ]\n",
        "\n",
        "        for pos_type, count in analysis['position_types'].items():\n",
        "            analysis_data.append([pos_type, count])\n",
        "\n",
        "        analysis_data.extend([\n",
        "            ['', ''],\n",
        "            ['GEOGRAPHIC DISTRIBUTION', ''],\n",
        "        ])\n",
        "\n",
        "        for country, count in sorted(analysis['geographic_distribution'].items(),\n",
        "                                   key=lambda x: x[1], reverse=True):\n",
        "            analysis_data.append([country, count])\n",
        "\n",
        "        analysis_data.extend([\n",
        "            ['', ''],\n",
        "            ['INSTITUTION PRESTIGE ANALYSIS', ''],\n",
        "            ['Top Tier Universities', analysis['institution_prestige']['top_tier']],\n",
        "            ['Mid Tier Universities', analysis['institution_prestige']['mid_tier']],\n",
        "            ['Other Institutions', analysis['institution_prestige']['other']],\n",
        "            ['', ''],\n",
        "            ['CAREER LEVEL OPPORTUNITIES', ''],\n",
        "            ['Assistant Professor Positions', len(df_assistant_prof)],\n",
        "            ['Lecturer Positions', len(df_lecturer)],\n",
        "            ['', ''],\n",
        "            ['REGIONAL OPPORTUNITIES', ''],\n",
        "            ['UK Positions', len(df_uk_positions)],\n",
        "            ['US Positions', len(df_us_positions)],\n",
        "            ['Asia-Pacific Positions', len(df_asia_positions)]\n",
        "        ])\n",
        "\n",
        "        df_analysis = pd.DataFrame(analysis_data, columns=['Metric', 'Value'])\n",
        "\n",
        "        # Create Excel workbook with multiple sheets\n",
        "        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
        "            # Main sheets\n",
        "            df_all.to_excel(writer, sheet_name='All Positions', index=False)\n",
        "\n",
        "            if not df_high_priority.empty:\n",
        "                df_high_priority.to_excel(writer, sheet_name='High Priority', index=False)\n",
        "\n",
        "            if not df_medium_priority.empty:\n",
        "                df_medium_priority.to_excel(writer, sheet_name='Medium Priority', index=False)\n",
        "\n",
        "            if not df_low_priority.empty:\n",
        "                df_low_priority.to_excel(writer, sheet_name='Low Priority', index=False)\n",
        "\n",
        "            # Specialized filters\n",
        "            if not df_target_institutions.empty:\n",
        "                df_target_institutions.to_excel(writer, sheet_name='Target Universities', index=False)\n",
        "\n",
        "            if not df_assistant_prof.empty:\n",
        "                df_assistant_prof.to_excel(writer, sheet_name='Assistant Professor', index=False)\n",
        "\n",
        "            if not df_lecturer.empty:\n",
        "                df_lecturer.to_excel(writer, sheet_name='Lecturer Positions', index=False)\n",
        "\n",
        "            # Geographic sheets\n",
        "            if not df_uk_positions.empty:\n",
        "                df_uk_positions.to_excel(writer, sheet_name='UK Positions', index=False)\n",
        "\n",
        "            if not df_us_positions.empty:\n",
        "                df_us_positions.to_excel(writer, sheet_name='US Positions', index=False)\n",
        "\n",
        "            if not df_asia_positions.empty:\n",
        "                df_asia_positions.to_excel(writer, sheet_name='Asia-Pacific', index=False)\n",
        "\n",
        "            # Analysis summary\n",
        "            df_analysis.to_excel(writer, sheet_name='Market Analysis', index=False)\n",
        "\n",
        "            # Format all sheets\n",
        "            workbook = writer.book\n",
        "            for sheet_name in workbook.sheetnames:\n",
        "                worksheet = workbook[sheet_name]\n",
        "\n",
        "                # Auto-adjust column widths\n",
        "                for column in worksheet.columns:\n",
        "                    max_length = 0\n",
        "                    column_letter = column[0].column_letter\n",
        "\n",
        "                    for cell in column:\n",
        "                        try:\n",
        "                            if len(str(cell.value)) > max_length:\n",
        "                                max_length = len(str(cell.value))\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    adjusted_width = min(max_length + 2, 60)  # Max width 60\n",
        "                    worksheet.column_dimensions[column_letter].width = adjusted_width\n",
        "\n",
        "                # Add filters to data sheets (except analysis)\n",
        "                if sheet_name != 'Market Analysis' and worksheet.max_row > 1:\n",
        "                    worksheet.auto_filter.ref = worksheet.dimensions\n",
        "\n",
        "        print(f\"Excel file created: {excel_filename}\")\n",
        "        print(f\"Sheets created:\")\n",
        "        sheet_info = [\n",
        "            f\"   All Positions ({len(df_all)} jobs)\",\n",
        "            f\"   High Priority ({len(df_high_priority)} jobs)\",\n",
        "            f\"   Medium Priority ({len(df_medium_priority)} jobs)\",\n",
        "            f\"   Target Universities ({len(df_target_institutions)} jobs)\",\n",
        "            f\"   Assistant Professor ({len(df_assistant_prof)} jobs)\",\n",
        "            f\"   Geographic breakdowns\",\n",
        "            f\"   Market Analysis summary\"\n",
        "        ]\n",
        "        for info in sheet_info:\n",
        "            print(info)\n",
        "\n",
        "        return excel_filename\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating Excel file: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def create_quick_summary_report(scraper):\n",
        "    \"\"\"Create a concise text summary of job search results\"\"\"\n",
        "\n",
        "    if not scraper.jobs_found:\n",
        "        return \"No jobs found to summarize.\"\n",
        "\n",
        "    total_jobs = len(scraper.jobs_found)\n",
        "    high_priority = [job for job in scraper.jobs_found if job.relevance_score >= 40]\n",
        "    medium_priority = [job for job in scraper.jobs_found if 25 <= job.relevance_score < 40]\n",
        "\n",
        "    # Geographic and institutional analysis\n",
        "    country_counts = {}\n",
        "    institution_counts = {}\n",
        "    for job in scraper.jobs_found:\n",
        "        country_counts[job.country] = country_counts.get(job.country, 0) + 1\n",
        "        if job.institution in ALL_TARGET_UNIVERSITIES:\n",
        "            institution_counts[job.institution] = institution_counts.get(job.institution, 0) + 1\n",
        "\n",
        "    report = f\"\"\"\n",
        "ACADEMIC JOB SEARCH SUMMARY REPORT\n",
        "Sociology PhD Position Search\n",
        "{'='*50}\n",
        "\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "Research Focus: Family Studies, Childhood, Care Work\n",
        "\n",
        "SEARCH RESULTS OVERVIEW\n",
        "{'='*50}\n",
        "Total Positions Found: {total_jobs}\n",
        "High Priority (Score 40+): {len(high_priority)}\n",
        "Medium Priority (Score 25-39): {len(medium_priority)}\n",
        "Low Priority (Score 15-24): {total_jobs - len(high_priority) - len(medium_priority)}\n",
        "\n",
        "TOP HIGH PRIORITY POSITIONS\n",
        "{'='*35}\"\"\"\n",
        "\n",
        "    for i, job in enumerate(high_priority[:8], 1):\n",
        "        report += f\"\"\"\n",
        "{i}. {job.title}\n",
        "   Institution: {job.institution}\n",
        "   Location: {job.location}, {job.country}\n",
        "   Relevance Score: {job.relevance_score}\n",
        "   Deadline: {job.deadline}\n",
        "   URL: {job.url[:60]}{'...' if len(job.url) > 60 else ''}\n",
        "\"\"\"\n",
        "\n",
        "    if len(high_priority) > 8:\n",
        "        report += f\"\\n   ... and {len(high_priority) - 8} more high priority positions\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "\n",
        "GEOGRAPHIC DISTRIBUTION\n",
        "{'='*25}\"\"\"\n",
        "    for country, count in sorted(country_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        report += f\"\\n{country}: {count} positions\"\n",
        "\n",
        "    if institution_counts:\n",
        "        report += f\"\"\"\n",
        "\n",
        "TOP TARGET INSTITUTIONS\n",
        "{'='*26}\"\"\"\n",
        "        for inst, count in sorted(institution_counts.items(), key=lambda x: x[1], reverse=True)[:8]:\n",
        "            report += f\"\\n{inst}: {count} positions\"\n",
        "\n",
        "    # Career-specific recommendations\n",
        "    assistant_prof_count = len([job for job in high_priority + medium_priority\n",
        "                               if 'assistant professor' in job.title.lower()])\n",
        "    lecturer_count = len([job for job in high_priority + medium_priority\n",
        "                         if 'lecturer' in job.title.lower()])\n",
        "\n",
        "    report += f\"\"\"\n",
        "\n",
        "STRATEGIC RECOMMENDATIONS\n",
        "{'='*27}\n",
        "• Priority Focus: Apply to {len(high_priority)} high-priority positions first\n",
        "• Assistant Professor Opportunities: {assistant_prof_count} positions available\n",
        "• Lecturer Track Options: {lecturer_count} positions for alternative pathway\n",
        "• Geographic Strategy: Consider {list(country_counts.keys())[:3]} as primary targets\n",
        "• Research Fit: Positions specifically match family/childhood expertise\n",
        "• Application Timeline: Review deadlines and prioritize urgent applications\n",
        "\n",
        "NEXT STEPS\n",
        "{'='*12}\n",
        "1. Focus on high-priority positions (score 40+)\n",
        "2. Customize applications highlighting family studies expertise\n",
        "3. Leverage China research experience for Asia-Pacific positions\n",
        "4. Create application timeline based on deadlines\n",
        "5. Access full details in Excel export for comprehensive planning\n",
        "\"\"\"\n",
        "\n",
        "    return report\n",
        "\n",
        "def generate_application_timeline(scraper, weeks_ahead=16):\n",
        "    \"\"\"Generate strategic application timeline based on deadlines and priorities\"\"\"\n",
        "\n",
        "    timeline_jobs = []\n",
        "    current_date = datetime.now()\n",
        "\n",
        "    for job in scraper.jobs_found:\n",
        "        if job.deadline == \"N/A\" or job.relevance_score < 20:\n",
        "            continue\n",
        "\n",
        "        # Estimate deadline urgency based on common patterns\n",
        "        deadline_lower = job.deadline.lower()\n",
        "        weeks_until = None\n",
        "\n",
        "        # Simple deadline categorization\n",
        "        if any(urgent in deadline_lower for urgent in ['asap', 'immediate', 'urgent']):\n",
        "            weeks_until = 1\n",
        "        elif any(soon in deadline_lower for soon in ['january', 'jan', 'february', 'feb']):\n",
        "            weeks_until = 4\n",
        "        elif any(medium in deadline_lower for medium in ['march', 'mar', 'april', 'apr']):\n",
        "            weeks_until = 8\n",
        "        elif any(later in deadline_lower for later in ['may', 'june', 'jul']):\n",
        "            weeks_until = 12\n",
        "        else:\n",
        "            weeks_until = 16  # Unknown, assume later\n",
        "\n",
        "        if weeks_until <= weeks_ahead:\n",
        "            urgency = 'URGENT' if weeks_until <= 2 else 'SOON' if weeks_until <= 6 else 'UPCOMING'\n",
        "            timeline_jobs.append({\n",
        "                'job': job,\n",
        "                'weeks_until': weeks_until,\n",
        "                'urgency': urgency\n",
        "            })\n",
        "\n",
        "    # Sort by urgency and then by relevance score\n",
        "    timeline_jobs.sort(key=lambda x: (x['weeks_until'], -x['job'].relevance_score))\n",
        "\n",
        "    if not timeline_jobs:\n",
        "        return \"No specific deadlines identified. Focus on high-priority positions and create your own timeline.\"\n",
        "\n",
        "    timeline_report = f\"\"\"\n",
        "APPLICATION TIMELINE STRATEGY\n",
        "Next {weeks_ahead} Weeks Planning\n",
        "{'='*50}\n",
        "\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "URGENT APPLICATIONS (Next 2 weeks)\n",
        "{'='*35}\"\"\"\n",
        "\n",
        "    urgent_jobs = [job_data for job_data in timeline_jobs if job_data['urgency'] == 'URGENT']\n",
        "    if urgent_jobs:\n",
        "        for job_data in urgent_jobs[:5]:  # Top 5 urgent\n",
        "            job = job_data['job']\n",
        "            timeline_report += f\"\"\"\n",
        "{job.title}\n",
        "  Institution: {job.institution} | {job.country}\n",
        "  Score: {job.relevance_score} | Deadline: {job.deadline}\n",
        "  URL: {job.url}\n",
        "\"\"\"\n",
        "    else:\n",
        "        timeline_report += \"\\nNo urgent deadlines identified\"\n",
        "\n",
        "    timeline_report += f\"\"\"\n",
        "\n",
        "PRIORITY APPLICATIONS (Next 2-6 weeks)\n",
        "{'='*40}\"\"\"\n",
        "\n",
        "    soon_jobs = [job_data for job_data in timeline_jobs if job_data['urgency'] == 'SOON']\n",
        "    if soon_jobs:\n",
        "        for job_data in soon_jobs[:6]:  # Top 6 priority\n",
        "            job = job_data['job']\n",
        "            timeline_report += f\"\"\"\n",
        "{job.title}\n",
        "  Institution: {job.institution} | Score: {job.relevance_score}\n",
        "  Estimated timeline: {job_data['weeks_until']} weeks\n",
        "\"\"\"\n",
        "    else:\n",
        "        timeline_report += \"\\nCheck individual job listings for specific deadlines\"\n",
        "\n",
        "    timeline_report += f\"\"\"\n",
        "\n",
        "UPCOMING OPPORTUNITIES (6+ weeks)\n",
        "{'='*35}\"\"\"\n",
        "\n",
        "    upcoming_jobs = [job_data for job_data in timeline_jobs if job_data['urgency'] == 'UPCOMING']\n",
        "    for job_data in upcoming_jobs[:8]:  # Top 8 upcoming\n",
        "        job = job_data['job']\n",
        "        timeline_report += f\"\\n{job.title} at {job.institution} (Score: {job.relevance_score})\"\n",
        "\n",
        "    if len(upcoming_jobs) > 8:\n",
        "        timeline_report += f\"\\n   ... and {len(upcoming_jobs) - 8} more upcoming positions\"\n",
        "\n",
        "    timeline_report += f\"\"\"\n",
        "\n",
        "STRATEGIC APPLICATION PLAN\n",
        "{'='*28}\n",
        "Week 1-2:  Focus on {len(urgent_jobs)} urgent applications\n",
        "Week 3-6:  Submit {len(soon_jobs)} priority applications\n",
        "Week 7+:   Prepare for {len(upcoming_jobs)} upcoming deadlines\n",
        "\n",
        "Application Preparation Checklist:\n",
        "• Update CV with recent research and publications\n",
        "• Prepare research statement highlighting family/childhood expertise\n",
        "• Draft teaching philosophy emphasizing sociology pedagogy\n",
        "• Secure recommendation letters from supervisors\n",
        "• Tailor cover letters for each institution and position\n",
        "• Prepare for potential video interviews or presentations\n",
        "\"\"\"\n",
        "\n",
        "    return timeline_report\n",
        "\n",
        "print(\"EXPORT AND REPORTING SYSTEM CONFIGURED\")\n",
        "print(\"Excel export with multiple analysis sheets\")\n",
        "print(\"Quick summary report generation\")\n",
        "print(\"Strategic application timeline creation\")\n",
        "print(\"Ready to generate comprehensive job search reports!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCJt9qVmIb7E",
        "outputId": "a7a20bbd-a5e4-4544-e555-46702af7394d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPORT AND REPORTING SYSTEM CONFIGURED\n",
            "Excel export with multiple analysis sheets\n",
            "Quick summary report generation\n",
            "Strategic application timeline creation\n",
            "Ready to generate comprehensive job search reports!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 10: Complete Execution and Interactive Interface\n",
        "# Main execution functions and user interface\n",
        "\n",
        "def create_personalized_recommendations(scraper):\n",
        "    \"\"\"\n",
        "    Create highly personalized recommendations based on Oxford sociology PhD profile\n",
        "    Tailored for family studies, childhood, and care work expertise\n",
        "    \"\"\"\n",
        "\n",
        "    # Categorize positions by career track\n",
        "    assistant_prof_positions = []\n",
        "    lecturer_positions = []\n",
        "    postdoc_positions = []\n",
        "    research_positions = []\n",
        "\n",
        "    for job in scraper.jobs_found:\n",
        "        title_lower = job.title.lower()\n",
        "\n",
        "        if 'postdoc' in title_lower or 'post-doc' in title_lower:\n",
        "            postdoc_positions.append(job)\n",
        "        elif 'assistant professor' in title_lower:\n",
        "            assistant_prof_positions.append(job)\n",
        "        elif 'lecturer' in title_lower or 'senior lecturer' in title_lower:\n",
        "            lecturer_positions.append(job)\n",
        "        elif 'research' in title_lower and job.relevance_score >= 20:\n",
        "            research_positions.append(job)\n",
        "\n",
        "    # Sort each category by relevance score\n",
        "    for positions in [assistant_prof_positions, lecturer_positions, postdoc_positions, research_positions]:\n",
        "        positions.sort(key=lambda x: x.relevance_score, reverse=True)\n",
        "\n",
        "    recommendations = f\"\"\"\n",
        "PERSONALIZED CAREER RECOMMENDATIONS\n",
        "Oxford Sociology PhD - Family & Childhood Studies\n",
        "{'='*50}\n",
        "\n",
        "Profile: Oxford Sociology PhD\n",
        "Expertise: Family Studies, Childhood, Care Work, China Research\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "PRIMARY TARGET: ASSISTANT PROFESSOR POSITIONS\n",
        "{'='*50}\"\"\"\n",
        "\n",
        "    if assistant_prof_positions:\n",
        "        recommendations += f\"\\nFound {len(assistant_prof_positions)} assistant professor opportunities:\\n\"\n",
        "        for i, job in enumerate(assistant_prof_positions[:6], 1):\n",
        "            tier = \"TOP TIER\" if job.relevance_score >= 45 else \"HIGH FIT\" if job.relevance_score >= 35 else \"GOOD FIT\"\n",
        "            recommendations += f\"\"\"\n",
        "{i}. {tier} | Score: {job.relevance_score}\n",
        "   {job.title}\n",
        "   Institution: {job.institution}\n",
        "   Location: {job.location}, {job.country}\n",
        "   Deadline: {job.deadline}\n",
        "   URL: {job.url}\n",
        "\"\"\"\n",
        "        if len(assistant_prof_positions) > 6:\n",
        "            recommendations += f\"\\n   ... plus {len(assistant_prof_positions) - 6} more assistant professor positions\"\n",
        "    else:\n",
        "        recommendations += \"\\nNo assistant professor positions found in current search\"\n",
        "\n",
        "    recommendations += f\"\"\"\n",
        "\n",
        "ALTERNATIVE TRACK: LECTURER POSITIONS\n",
        "{'='*43}\"\"\"\n",
        "\n",
        "    if lecturer_positions:\n",
        "        recommendations += f\"\\nFound {len(lecturer_positions)} lecturer opportunities:\\n\"\n",
        "        for i, job in enumerate(lecturer_positions[:4], 1):\n",
        "            recommendations += f\"\"\"\n",
        "{i}. {job.title}\n",
        "   Institution: {job.institution} | {job.country}\n",
        "   Score: {job.relevance_score} | Deadline: {job.deadline}\n",
        "\"\"\"\n",
        "    else:\n",
        "        recommendations += \"\\nLimited lecturer positions found\"\n",
        "\n",
        "    recommendations += f\"\"\"\n",
        "\n",
        "RESEARCH DEVELOPMENT: POSTDOC OPPORTUNITIES\n",
        "{'='*48}\"\"\"\n",
        "\n",
        "    if postdoc_positions:\n",
        "        recommendations += f\"\\nFound {len(postdoc_positions)} postdoc opportunities:\\n\"\n",
        "        for i, job in enumerate(postdoc_positions[:4], 1):\n",
        "            recommendations += f\"\"\"\n",
        "{i}. {job.title}\n",
        "   Institution: {job.institution} | {job.country}\n",
        "   Score: {job.relevance_score}\n",
        "\"\"\"\n",
        "    else:\n",
        "        recommendations += \"\\nConsider searching specialized postdoc databases\"\n",
        "\n",
        "    # Geographic analysis with strategic recommendations\n",
        "    uk_positions = [job for job in scraper.jobs_found if job.country == \"United Kingdom\" and job.relevance_score >= 25]\n",
        "    us_positions = [job for job in scraper.jobs_found if job.country == \"United States\" and job.relevance_score >= 25]\n",
        "    asia_positions = [job for job in scraper.jobs_found if job.country in [\"Hong Kong\", \"Singapore\", \"China\"] and job.relevance_score >= 20]\n",
        "\n",
        "    recommendations += f\"\"\"\n",
        "\n",
        "STRATEGIC GEOGRAPHIC ANALYSIS\n",
        "{'='*35}\n",
        "\n",
        "UNITED KINGDOM ({len(uk_positions)} opportunities)\n",
        "   Advantages: Familiar system, post-study visa, research networks\n",
        "   Top Matches:\"\"\"\n",
        "\n",
        "    if uk_positions:\n",
        "        top_uk = sorted(uk_positions, key=lambda x: x.relevance_score, reverse=True)[:3]\n",
        "        for job in top_uk:\n",
        "            recommendations += f\"\\n   {job.title} at {job.institution} (Score: {job.relevance_score})\"\n",
        "    else:\n",
        "        recommendations += \"\\n   Limited high-relevance UK positions found\"\n",
        "\n",
        "    recommendations += f\"\"\"\n",
        "\n",
        "UNITED STATES ({len(us_positions)} opportunities)\n",
        "   Advantages: Large academic market, research funding, career growth\n",
        "   Focus Areas: Family sociology programs, childhood studies centers\"\"\"\n",
        "\n",
        "    if us_positions:\n",
        "        top_us = sorted(us_positions, key=lambda x: x.relevance_score, reverse=True)[:3]\n",
        "        for job in top_us:\n",
        "            recommendations += f\"\\n   {job.title} at {job.institution} (Score: {job.relevance_score})\"\n",
        "\n",
        "    recommendations += f\"\"\"\n",
        "\n",
        "ASIA-PACIFIC ({len(asia_positions)} opportunities)\n",
        "   Advantages: Regional expertise, language skills, cultural knowledge\n",
        "   Strategic Value: Leverage China research background\"\"\"\n",
        "\n",
        "    if asia_positions:\n",
        "        top_asia = sorted(asia_positions, key=lambda x: x.relevance_score, reverse=True)[:3]\n",
        "        for job in top_asia:\n",
        "            recommendations += f\"\\n   {job.title} at {job.institution} (Score: {job.relevance_score})\"\n",
        "\n",
        "    # Research specialization recommendations\n",
        "    family_positions = [job for job in scraper.jobs_found if any(keyword in job.title.lower()\n",
        "                       for keyword in ['family', 'childhood', 'care']) and job.relevance_score >= 30]\n",
        "\n",
        "    recommendations += f\"\"\"\n",
        "\n",
        "RESEARCH SPECIALIZATION MATCHES\n",
        "{'='*35}\n",
        "Family/Childhood Expertise Positions: {len(family_positions)}\"\"\"\n",
        "\n",
        "    if family_positions:\n",
        "        for job in family_positions[:3]:\n",
        "            recommendations += f\"\\n{job.title} at {job.institution} (Score: {job.relevance_score})\"\n",
        "\n",
        "    recommendations += f\"\"\"\n",
        "\n",
        "STRATEGIC ACTION PLAN\n",
        "{'='*23}\n",
        "IMMEDIATE PRIORITIES (Next 2 weeks):\n",
        "1. Apply to top 3 assistant professor positions (scores 40+)\n",
        "2. Tailor research statements to emphasize family studies expertise\n",
        "3. Network with faculty at target institutions\n",
        "\n",
        "SHORT-TERM GOALS (Next 1-2 months):\n",
        "4. Submit applications to high-fit lecturer positions\n",
        "5. Explore Asia-Pacific opportunities leveraging China expertise\n",
        "6. Consider collaborative research opportunities\n",
        "\n",
        "LONG-TERM STRATEGY (Next 6 months):\n",
        "7. Continue publishing in family sociology and childhood studies\n",
        "8. Present research at major sociology conferences\n",
        "9. Expand search to include emerging positions\n",
        "\n",
        "PROFILE OPTIMIZATION RECOMMENDATIONS\n",
        "{'='*37}\n",
        "• Emphasize quantitative and qualitative mixed-methods expertise\n",
        "• Highlight cross-cultural research experience (China focus)\n",
        "• Showcase policy implications of family and care work research\n",
        "• Develop teaching portfolio in sociology of family and childhood\n",
        "• Build connections with family studies and childhood research networks\n",
        "\n",
        "SUCCESS PROBABILITY ANALYSIS\n",
        "{'='*30}\n",
        "High-fit positions (40+ score): {len([j for j in scraper.jobs_found if j.relevance_score >= 40])} - EXCELLENT chances\n",
        "Medium-fit positions (25-39): {len([j for j in scraper.jobs_found if 25 <= j.relevance_score < 40])} - GOOD chances\n",
        "Consider positions (15-24): {len([j for j in scraper.jobs_found if 15 <= j.relevance_score < 25])} - POSSIBLE chances\n",
        "\n",
        "Focus your energy on high and medium-fit positions for optimal results!\n",
        "\"\"\"\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "def main_job_search_execution():\n",
        "    \"\"\"\n",
        "    Execute complete academic job search with full reporting\n",
        "    One-click solution for comprehensive job hunting\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\"\"\n",
        "    GLOBAL ACADEMIC JOB SEARCH SYSTEM\n",
        "    Sociology PhD Career Opportunities\n",
        "    Family Studies • Childhood • Care Work\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"INITIATING COMPREHENSIVE JOB SEARCH...\")\n",
        "    print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Initialize fresh scraper instance\n",
        "    global scraper\n",
        "    scraper = GlobalAcademicJobScraper()\n",
        "\n",
        "    try:\n",
        "        # Execute comprehensive search\n",
        "        print(\"\\nRUNNING COMPREHENSIVE SEARCH ACROSS ALL PLATFORMS...\")\n",
        "        jobs = run_comprehensive_job_search(scraper, comprehensive=True)\n",
        "\n",
        "        if not jobs:\n",
        "            print(\"No jobs found. Please check your internet connection and try again.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"\\nSEARCH COMPLETED! Found {len(jobs)} total positions\")\n",
        "\n",
        "        # Generate all reports and exports\n",
        "        print(\"\\nGENERATING COMPREHENSIVE REPORTS...\")\n",
        "\n",
        "        # 1. Excel Export (primary output)\n",
        "        print(\"\\nCreating Excel workbook...\")\n",
        "        excel_file = save_results(scraper, \"oxford_sociology_academic_jobs\")\n",
        "\n",
        "        # 2. Quick Summary Report\n",
        "        print(\"\\nGenerating summary report...\")\n",
        "        summary_report = create_quick_summary_report(scraper)\n",
        "\n",
        "        # 3. Application Timeline\n",
        "        print(\"\\nCreating application timeline...\")\n",
        "        timeline_report = generate_application_timeline(scraper)\n",
        "\n",
        "        # 4. Personalized Recommendations\n",
        "        print(\"\\nDeveloping personalized recommendations...\")\n",
        "        recommendations = create_personalized_recommendations(scraper)\n",
        "\n",
        "        # Save all text reports\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Save summary\n",
        "        summary_filename = f\"job_search_summary_{timestamp}.txt\"\n",
        "        with open(summary_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(summary_report)\n",
        "\n",
        "        # Save timeline\n",
        "        timeline_filename = f\"application_timeline_{timestamp}.txt\"\n",
        "        with open(timeline_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(timeline_report)\n",
        "\n",
        "        # Save recommendations\n",
        "        rec_filename = f\"career_recommendations_{timestamp}.txt\"\n",
        "        with open(rec_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(recommendations)\n",
        "\n",
        "        # Display results\n",
        "        print(summary_report)\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"JOB SEARCH COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(f\"\\nFILES GENERATED AND READY FOR DOWNLOAD:\")\n",
        "        print(f\"   {excel_file} - Complete job database with analysis\")\n",
        "        print(f\"   {summary_filename} - Executive summary report\")\n",
        "        print(f\"   {timeline_filename} - Strategic application timeline\")\n",
        "        print(f\"   {rec_filename} - Personalized career recommendations\")\n",
        "\n",
        "        # Final statistics\n",
        "        high_priority_count = len([j for j in jobs if j.relevance_score >= 40])\n",
        "        medium_priority_count = len([j for j in jobs if 25 <= j.relevance_score < 40])\n",
        "\n",
        "        print(f\"\\nFINAL SEARCH STATISTICS:\")\n",
        "        print(f\"   High Priority Positions: {high_priority_count}\")\n",
        "        print(f\"   Medium Priority Positions: {medium_priority_count}\")\n",
        "        print(f\"   Countries Covered: {len(set(job.country for job in jobs))}\")\n",
        "        print(f\"   Institutions Searched: {len(set(job.institution for job in jobs))}\")\n",
        "        print(f\"   Search Duration: {datetime.now().strftime('%H:%M:%S')}\")\n",
        "\n",
        "        print(f\"\\nNEXT STEPS:\")\n",
        "        print(f\"   1. Download and review the Excel file for complete job details\")\n",
        "        print(f\"   2. Follow the personalized recommendations for strategic applications\")\n",
        "        print(f\"   3. Use the timeline to prioritize urgent applications\")\n",
        "        print(f\"   4. Customize cover letters and research statements for top positions\")\n",
        "\n",
        "        return {\n",
        "            'jobs': jobs,\n",
        "            'excel_file': excel_file,\n",
        "            'summary_file': summary_filename,\n",
        "            'timeline_file': timeline_filename,\n",
        "            'recommendations_file': rec_filename,\n",
        "            'statistics': {\n",
        "                'total_jobs': len(jobs),\n",
        "                'high_priority': high_priority_count,\n",
        "                'medium_priority': medium_priority_count,\n",
        "                'countries': len(set(job.country for job in jobs)),\n",
        "                'institutions': len(set(job.institution for job in jobs))\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nSearch interrupted by user. Partial results may be available.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during job search: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def quick_demo_search():\n",
        "    \"\"\"Quick demonstration search for testing purposes\"\"\"\n",
        "\n",
        "    print(\"RUNNING QUICK DEMO SEARCH\")\n",
        "    print(\"=\"*30)\n",
        "    print(\"Testing scraper functionality with limited search...\")\n",
        "\n",
        "    demo_scraper = GlobalAcademicJobScraper()\n",
        "\n",
        "    # Run limited search for testing\n",
        "    print(\"\\nSearching Jobs.ac.uk for sociology positions...\")\n",
        "    demo_jobs = scrape_jobs_ac_uk(demo_scraper, keywords=[\"sociology\"], max_pages=1)\n",
        "\n",
        "    if demo_jobs:\n",
        "        print(f\"\\nDemo successful! Found {len(demo_jobs)} positions.\")\n",
        "        print(\"\\nTop positions from demo:\")\n",
        "        for i, job in enumerate(demo_jobs[:3], 1):\n",
        "            priority = get_priority_tier(job.relevance_score)\n",
        "            print(f\"   {i}. {priority} | Score: {job.relevance_score}\")\n",
        "            print(f\"      {job.title}\")\n",
        "            print(f\"      {job.institution} | {job.country}\")\n",
        "\n",
        "        # Quick export\n",
        "        excel_file = save_results(demo_scraper, \"demo_sociology_jobs\")\n",
        "        summary = create_quick_summary_report(demo_scraper)\n",
        "\n",
        "        print(f\"\\nDemo results exported to: {excel_file}\")\n",
        "        print(\"\\nDemo Summary:\")\n",
        "        print(summary[:500] + \"...\" if len(summary) > 500 else summary)\n",
        "\n",
        "    else:\n",
        "        print(\"No positions found in demo search. Check internet connection.\")\n",
        "\n",
        "    return demo_jobs\n",
        "\n",
        "# MAIN EXECUTION COMMANDS\n",
        "print(\"\"\"\n",
        "ACADEMIC JOB SCRAPER READY!\n",
        "\n",
        "EXECUTION OPTIONS:\n",
        "\n",
        "FULL COMPREHENSIVE SEARCH (Recommended):\n",
        "   results = main_job_search_execution()\n",
        "\n",
        "QUICK DEMO (For Testing):\n",
        "   demo_results = quick_demo_search()\n",
        "\n",
        "MANUAL STEP-BY-STEP:\n",
        "   jobs = run_comprehensive_job_search(scraper)\n",
        "   excel_file = save_results(scraper)\n",
        "   summary = create_quick_summary_report(scraper)\n",
        "\n",
        "The system will automatically:\n",
        "• Search 6+ major academic job platforms worldwide\n",
        "• Score positions based on your sociology PhD expertise\n",
        "• Filter for family studies, childhood, and care work relevance\n",
        "• Export comprehensive Excel workbook with multiple analysis sheets\n",
        "• Generate personalized career recommendations\n",
        "• Create strategic application timeline\n",
        "• Save all reports as downloadable files\n",
        "\n",
        "Optimized for sociology PhD with China expertise\n",
        "Covers US, UK, Europe, and Asia-Pacific opportunities\n",
        "Intelligent relevance scoring for maximum efficiency\n",
        "\n",
        "READY TO LAUNCH YOUR ACADEMIC CAREER SEARCH!\n",
        "\"\"\")\n",
        "\n",
        "# Auto-run demo for immediate testing (comment out for manual control)\n",
        "# Uncomment the line below to run demo automatically:\n",
        "# demo_results = quick_demo_search()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dl9XoENIb-D",
        "outputId": "58dc2007-bc00-4eba-8ae3-55144d09f3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ACADEMIC JOB SCRAPER READY!\n",
            "\n",
            "EXECUTION OPTIONS:\n",
            "\n",
            "FULL COMPREHENSIVE SEARCH (Recommended):\n",
            "   results = main_job_search_execution()\n",
            "\n",
            "QUICK DEMO (For Testing):\n",
            "   demo_results = quick_demo_search()\n",
            "\n",
            "MANUAL STEP-BY-STEP:\n",
            "   jobs = run_comprehensive_job_search(scraper)\n",
            "   excel_file = save_results(scraper)\n",
            "   summary = create_quick_summary_report(scraper)\n",
            "\n",
            "The system will automatically:\n",
            "• Search 6+ major academic job platforms worldwide\n",
            "• Score positions based on your sociology PhD expertise  \n",
            "• Filter for family studies, childhood, and care work relevance\n",
            "• Export comprehensive Excel workbook with multiple analysis sheets\n",
            "• Generate personalized career recommendations\n",
            "• Create strategic application timeline\n",
            "• Save all reports as downloadable files\n",
            "\n",
            "Optimized for sociology PhD with China expertise\n",
            "Covers US, UK, Europe, and Asia-Pacific opportunities\n",
            "Intelligent relevance scoring for maximum efficiency\n",
            "\n",
            "READY TO LAUNCH YOUR ACADEMIC CAREER SEARCH!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_results = quick_demo_search()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYSOr0Y3IcA4",
        "outputId": "6c0c7798-8b2a-4deb-8127-bd6ed97d6feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUNNING QUICK DEMO SEARCH\n",
            "==============================\n",
            "Testing scraper functionality with limited search...\n",
            "Academic job scraper initialized!\n",
            "\n",
            "Searching Jobs.ac.uk for sociology positions...\n",
            "STARTING JOBS.AC.UK SCRAPING\n",
            "==================================================\n",
            "\n",
            "Searching for: 'sociology'\n",
            "   Page 1\n",
            "   No jobs found on page 1\n",
            "   Found 25 job links via alternative method\n",
            "   Found 10 job listings\n",
            "Waiting 2.9 seconds...\n",
            "Waiting 3.3 seconds...\n",
            "\n",
            "JOBS.AC.UK SCRAPING COMPLETE\n",
            "Found 0 relevant positions\n",
            "High priority: 0\n",
            "Medium priority: 0\n",
            "No positions found in demo search. Check internet connection.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = main_job_search_execution()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Sy6kMvLKHEL",
        "outputId": "84245155-68bc-4827-f806-d6845652a58d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    GLOBAL ACADEMIC JOB SEARCH SYSTEM\n",
            "    Sociology PhD Career Opportunities\n",
            "    Family Studies • Childhood • Care Work\n",
            "    \n",
            "INITIATING COMPREHENSIVE JOB SEARCH...\n",
            "Start time: 2025-06-17 23:10:25\n",
            "Academic job scraper initialized!\n",
            "\n",
            "RUNNING COMPREHENSIVE SEARCH ACROSS ALL PLATFORMS...\n",
            "STARTING COMPREHENSIVE ACADEMIC JOB SEARCH\n",
            "============================================================\n",
            "Target: Sociology positions at top universities worldwide\n",
            "Research focus: Family studies, childhood, care work\n",
            "Level: Oxford sociology PhD\n",
            "Search time: 2025-06-17 23:10:25\n",
            "Comprehensive mode: True\n",
            "============================================================\n",
            "\n",
            "PHASE 1: JOBS.AC.UK\n",
            "========================================\n",
            "STARTING JOBS.AC.UK SCRAPING\n",
            "==================================================\n",
            "\n",
            "Searching for: 'sociology'\n",
            "   Page 1\n",
            "   No jobs found on page 1\n",
            "   Found 25 job links via alternative method\n",
            "   Found 10 job listings\n",
            "Waiting 2.9 seconds...\n",
            "   Page 2\n",
            "   No jobs found on page 2\n",
            "Waiting 3.2 seconds...\n",
            "\n",
            "Searching for: 'social science'\n",
            "   Page 1\n",
            "   No jobs found on page 1\n",
            "   Found 25 job links via alternative method\n",
            "   Found 10 job listings\n",
            "Waiting 3.9 seconds...\n",
            "   Page 2\n",
            "   No jobs found on page 2\n",
            "Waiting 4.2 seconds...\n",
            "\n",
            "Searching for: 'family studies'\n",
            "   Page 1\n",
            "   No jobs found on page 1\n",
            "   Found 25 job links via alternative method\n",
            "   Found 10 job listings\n",
            "Waiting 3.4 seconds...\n",
            "   Page 2\n",
            "   No jobs found on page 2\n",
            "Waiting 4.3 seconds...\n",
            "\n",
            "Searching for: 'childhood studies'\n",
            "   Page 1\n",
            "   No jobs found on page 1\n",
            "   Found 13 job links via alternative method\n",
            "   Found 10 job listings\n",
            "Waiting 3.3 seconds...\n",
            "   Page 2\n",
            "   No jobs found on page 2\n",
            "Waiting 3.5 seconds...\n",
            "\n",
            "JOBS.AC.UK SCRAPING COMPLETE\n",
            "Found 0 relevant positions\n",
            "High priority: 0\n",
            "Medium priority: 0\n",
            "\n",
            "PHASE 2: HIGHEREDJOBS.COM\n",
            "========================================\n",
            "STARTING HIGHEREDJOBS.COM SCRAPING\n",
            "==================================================\n",
            "\n",
            "Searching for: 'sociology'\n",
            "   Page 1\n",
            "   No jobs found on page 1\n",
            "Waiting 4.7 seconds...\n",
            "\n",
            "Searching for: 'social+science'\n",
            "   Page 1\n",
            "   No jobs found on page 1\n",
            "Waiting 3.4 seconds...\n",
            "\n",
            "HIGHEREDJOBS SCRAPING COMPLETE\n",
            "Found 0 relevant positions\n",
            "\n",
            "PHASE 3: TIMES HIGHER EDUCATION\n",
            "========================================\n",
            "STARTING TIMES HIGHER EDUCATION SCRAPING\n",
            "==================================================\n",
            "\n",
            "Searching for: 'sociology'\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to load THE Jobs search\n",
            "\n",
            "Searching for: 'social science'\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to load THE Jobs search\n",
            "\n",
            "Searching for: 'family studies'\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to load THE Jobs search\n",
            "\n",
            "Searching for: 'childhood studies'\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to load THE Jobs search\n",
            "\n",
            "TIMES HIGHER EDUCATION SCRAPING COMPLETE\n",
            "Found 0 relevant positions\n",
            "\n",
            "PHASE 4: ACADEMICPOSITIONS.COM\n",
            "========================================\n",
            "STARTING ACADEMICPOSITIONS.COM SCRAPING\n",
            "==================================================\n",
            "\n",
            "Searching for: 'sociology'\n",
            "   Page 1\n",
            "Request failed for https://academicpositions.com/ad/search?q=sociology&page=1: HTTPSConnectionPool(host='academicpositions.com', port=443): Max retries exceeded with url: /ad/search?q=sociology&page=1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "   Failed to load page 1\n",
            "   Page 2\n",
            "Request failed for https://academicpositions.com/ad/search?q=sociology&page=2: HTTPSConnectionPool(host='academicpositions.com', port=443): Max retries exceeded with url: /ad/search?q=sociology&page=2 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "   Failed to load page 2\n",
            "   Page 3\n",
            "Request failed for https://academicpositions.com/ad/search?q=sociology&page=3: HTTPSConnectionPool(host='academicpositions.com', port=443): Max retries exceeded with url: /ad/search?q=sociology&page=3 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "   Failed to load page 3\n",
            "Waiting 4.4 seconds...\n",
            "\n",
            "Searching for: 'social science'\n",
            "   Page 1\n",
            "Request failed for https://academicpositions.com/ad/search?q=social+science&page=1: HTTPSConnectionPool(host='academicpositions.com', port=443): Max retries exceeded with url: /ad/search?q=social+science&page=1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "   Failed to load page 1\n",
            "   Page 2\n",
            "Request failed for https://academicpositions.com/ad/search?q=social+science&page=2: HTTPSConnectionPool(host='academicpositions.com', port=443): Max retries exceeded with url: /ad/search?q=social+science&page=2 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "   Failed to load page 2\n",
            "   Page 3\n",
            "Request failed for https://academicpositions.com/ad/search?q=social+science&page=3: HTTPSConnectionPool(host='academicpositions.com', port=443): Max retries exceeded with url: /ad/search?q=social+science&page=3 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "   Failed to load page 3\n",
            "Waiting 3.1 seconds...\n",
            "\n",
            "Searching for: 'family studies'\n",
            "   Page 1\n",
            "Request failed for https://academicpositions.com/ad/search?q=family+studies&page=1: HTTPSConnectionPool(host='academicpositions.com', port=443): Max retries exceeded with url: /ad/search?q=family+studies&page=1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "   Failed to load page 1\n",
            "   Page 2\n",
            "Request failed for https://academicpositions.com/ad/search?q=family+studies&page=2: HTTPSConnectionPool(host='academicpositions.com', port=443): Max retries exceeded with url: /ad/search?q=family+studies&page=2 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "   Failed to load page 2\n",
            "   Page 3\n",
            "Request failed for https://academicpositions.com/ad/search?q=family+studies&page=3: HTTPSConnectionPool(host='academicpositions.com', port=443): Max retries exceeded with url: /ad/search?q=family+studies&page=3 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "   Failed to load page 3\n",
            "Waiting 4.8 seconds...\n",
            "\n",
            "Searching for: 'childhood studies'\n",
            "   Page 1\n",
            "Request failed for https://academicpositions.com/ad/search?q=childhood+studies&page=1: HTTPSConnectionPool(host='academicpositions.com', port=443): Max retries exceeded with url: /ad/search?q=childhood+studies&page=1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "   Failed to load page 1\n",
            "   Page 2\n",
            "Request failed for https://academicpositions.com/ad/search?q=childhood+studies&page=2: HTTPSConnectionPool(host='academicpositions.com', port=443): Max retries exceeded with url: /ad/search?q=childhood+studies&page=2 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "   Failed to load page 2\n",
            "   Page 3\n",
            "Request failed for https://academicpositions.com/ad/search?q=childhood+studies&page=3: HTTPSConnectionPool(host='academicpositions.com', port=443): Max retries exceeded with url: /ad/search?q=childhood+studies&page=3 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)')))\n",
            "   Failed to load page 3\n",
            "Waiting 4.9 seconds...\n",
            "\n",
            "ACADEMICPOSITIONS.COM SCRAPING COMPLETE\n",
            "Found 0 relevant positions\n",
            "\n",
            "PHASE 5: UNIVERSITY CAREER PAGES\n",
            "========================================\n",
            "STARTING UNIVERSITY CAREER PAGES SCRAPING\n",
            "==================================================\n",
            "\n",
            "Scraping Harvard University...\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to access Harvard University careers page\n",
            "\n",
            "Scraping Stanford University...\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to access Stanford University careers page\n",
            "\n",
            "Scraping University of Chicago...\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to access University of Chicago careers page\n",
            "\n",
            "Scraping Yale University...\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to access Yale University careers page\n",
            "\n",
            "Scraping Columbia University...\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to access Columbia University careers page\n",
            "\n",
            "Scraping University of Oxford...\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to access University of Oxford careers page\n",
            "\n",
            "Scraping University of Cambridge...\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to access University of Cambridge careers page\n",
            "\n",
            "Scraping University College London...\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to access University College London careers page\n",
            "\n",
            "Scraping London School of Economics...\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to access London School of Economics careers page\n",
            "\n",
            "Scraping University of Edinburgh...\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to access University of Edinburgh careers page\n",
            "\n",
            "Scraping University of Manchester...\n",
            "Chrome driver setup failed: WebDriver.__init__() got multiple values for argument 'options'\n",
            "   Failed to access University of Manchester careers page\n",
            "\n",
            "UNIVERSITY CAREER PAGES SCRAPING COMPLETE\n",
            "Found 0 relevant positions from university websites\n",
            "\n",
            "PHASE 6: ASIA-PACIFIC SPECIALIZATION\n",
            "========================================\n",
            "STARTING ASIA-PACIFIC POSITIONS SCRAPING\n",
            "==================================================\n",
            "\n",
            "Searching for Academic Jobs Singapore positions...\n",
            "   Found 0 positions for Academic Jobs Singapore\n",
            "\n",
            "Searching for Hong Kong Universities positions...\n",
            "   Found 0 positions for Hong Kong Universities\n",
            "\n",
            "Searching for Sino-Foreign Universities positions...\n",
            "   Found 0 positions for Sino-Foreign Universities\n",
            "\n",
            "ASIA-PACIFIC SCRAPING COMPLETE\n",
            "Identified 0 Asia-Pacific positions\n",
            "Singapore positions: 0\n",
            "Hong Kong positions: 0\n",
            "China/Sino-foreign positions: 0\n",
            "\n",
            "PROCESSING AND ANALYZING RESULTS\n",
            "========================================\n",
            "\n",
            "COMPREHENSIVE JOB SEARCH COMPLETE!\n",
            "==================================================\n",
            "SUMMARY STATISTICS:\n",
            "   Total unique positions found: 0\n",
            "   High priority (40+ score): 0\n",
            "   Medium priority (25-39 score): 0\n",
            "   Low priority (15-24 score): 0\n",
            "   Failed URLs: 12\n",
            "\n",
            "GEOGRAPHIC DISTRIBUTION:\n",
            "\n",
            "PLATFORM PERFORMANCE:\n",
            "\n",
            "TOP TARGET INSTITUTIONS:\n",
            "\n",
            "TOP 10 HIGHEST SCORING POSITIONS:\n",
            "\n",
            "SEARCH PHASE RESULTS:\n",
            "   Jobs.ac.uk: 0 positions\n",
            "   HigherEdJobs: 0 positions\n",
            "   Times Higher Education: 0 positions\n",
            "   AcademicPositions.com: 0 positions\n",
            "   University Careers: 0 positions\n",
            "   Asia-Pacific: 0 positions\n",
            "No jobs found. Please check your internet connection and try again.\n"
          ]
        }
      ]
    }
  ]
}